paper_id	affiliations
variani22_interspeech	['Google Inc.']
nozaki22_interspeech	['Graduate School of Informatics, Kyoto University, Japan', 'RevComm, Inc., Japan']
tae22_interspeech	['Yale University', 'Supertone, Inc.', 'Neosapience, Inc.']
kanda22_interspeech	['Microsoft Cloud+AI, USA', 'Microsoft Research Asia, China']
bousquet22_interspeech	['LIA - Avignon University']
wang22_interspeech	['School of Languages and Linguistics, University of Melbourne, Australia', 'MARCS Institute for Brain, Behaviour and Development, University of Western Sydney, Australia']
meng22_interspeech	['Microsoft Corporation, USA', 'Microsoft Research Asia, China']
saeed22_interspeech	['Dept. of Computer Science and Technology', 'University of Cambridge']
xu22_interspeech	['University of Science and Technology of China (USTC), Hefei, China', 'State Key Laboratory of Acoustics, Institute of Acoustics, Chinese Academy of Sciences, China']
yoo22_interspeech	['AI Lab, CTO Division, LG Electronics, Seoul, Republic of Korea']
peyser22_interspeech	['Cho1', 'Center for Data Science, New York University, New York City, USA', 'Google Inc., U.S.A']
turrisi22_interspeech	['University of Genoa', 'Italian Institute of Technology', 'University of Ferrara', ' PerVoice']
huang22_interspeech	['Tara N. Sainath, Cyril Allauzen, Cal Peyser, Zhiyun Lu', 'Google Research, USA']
li22_interspeech	['School of Computer and Control Engineering, Yantai University, Yantai, China', 'Key Laboratory of Child Development and Learning Science of Ministry of Education,', 'School of Biological Science and Medical Engineering, Southeast University, Nanjing, China']
schmidt22_interspeech	['Music Technology Group, Universitat Pompeu Fabra', 'Dolby Laboratories']
liang22_interspeech	['Key Innovation Group of Digital Humanities Resource and Research,', 'Shanghai Normal University, Shanghai, China', 'Unisound AI Technology Co., Ltd., Beijing, China']
ge22_interspeech	['Shanghai Normal University, Shanghai, China', 'Unisound AI Technology Co., Ltd., Beijing, China']
han22_interspeech	['MINDsLab Inc., Republic of Korea', 'Seoul National University, Republic of Korea']
cho22_interspeech	['MINDsLab Inc., Republic of Korea', 'Seoul National University, Republic of Korea', 'Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea']
lee22_interspeech	['Korea University, Republic of Korea']
wang22b_interspeech	"['College of Design and Innovation, Tongji University, China', 'School of Pop Music & Dance, Shanghai Institute of Visual Arts, China', ""Northwestern Polytechnical University, Xi'an, China"", 'Fuxi AI Lab, NetEase Inc., Hangzhou, China']"
strom22_interspeech	['Amazon Alexa AI', 'ABSTRACT', 'Inference with large deep learning models in resource-constrained', 'settings is increasingly a bottleneck in real-world applications of', 'state-of-the-art AI. Here we address this by low-precision weight', 'quantization.', 'We achieve very low accuracy degradation by re-', 'parameterizing the weights in a way that leaves the weight distribu-', 'tion approximately uniform. We show lower bit-width quantization', 'and less accuracy degradation than previously reported in experi-', 'ments on GLUE benchmarks (3-bit', '.2% rel. degradation), and on', 'internal intent/slot-filling datasets (2-bit', '.4% rel. degradation).', 'Index Terms: quantization, deep learning, transformers', '. INTRODUCTION', 'Very large machine learning models, with a high count of learn-', 'able parameters is a hallmark of contemporary deep learning. A', 'number of studies have shown and quantified that in many settings,', 'larger is better, e.g., [1]. In particular for Natural Language Pro-', 'cessing (NLP), large transformer-based language models trained on', 'terrabytes of unlabeled data have recently formed the backbone for', 'systems that excel on important benchmarks [2', ']. As the success', 'and size of such models grow, so too does the importance of com-', 'pressing them without sacrificing too much of their performance.', 'Compression is motivated by inference speed/cost, for enabling de-', 'ployment in resource-constrained edge devices, and a lower carbon', 'footprint [4', '].', 'Techniques for compressing neural network models range from', 'pruning weights [6', '], to low-rank factorization [8], to training a', 'small model under the supervision of a larger one [9]. One method', 'with a rich history is quantizing the model parameters into fewer', 'bits [10', ']. Quantization is a straight-forward approach for re-', 'ducing size and increasing speed and, with bit-widths of eight bits', 'or more, it can often be done easily with very little degradation in', 'accuracy [12]. However for a given model and task setting, each', 'quantization method has a characteristic accuracy versus compres-', 'sion curve where accuracy typically degrades rapidly above some', 'level of aggressive quantization. For transformer models in particu-', 'lar, it has been shown difficult to quantize weights to fewer than four', 'bits without significant degradation [13', '] and many works focus', 'on 8-bit quantization, e.g., [15].', 'A basic approach is full-precision training followed by quanti-', 'zation, but for aggressive compression the quantization noise intro-', 'duced by this mismatch between training and test can degrade ac-', 'curacy. This can be mitigated by quantization-aware training, [11],', 'and more recently [12], which introduces quantization during train-', 'ing, typically by injecting quantization noise in the forward pass,', 'and estimating the gradients of quantized parameters with Straight', 'Through Estimators (STE) [16]. Some studies have pointed out that', 'as a secondary effect, STE introduces a small bias during training', 'and have proposed extensions to the method [17].', 'A relatively unexplored approach to quantization-aware training', 'is studying and optimizing the overall distribution of weights under', 'the typical stochastic gradient descent (SGD) optimization. In this', 'paper we take this approach when we consider the problem of quan-', 'tizing large transformer-based language models into low bit-widths', '(sub 4-bit precision). This type of compression level promises to re-', 'duce memory footprint even further and allow very fast inference by', 'leveraging low bit precision operators.', 'Our contributions', 'We propose (i) a novel weight transforma-', 'tion that causes SGD to learn approximately uniformly distributed', 'weights instead of the typical Gaussian distribution. Further, we ap-', 'ply (ii) a regularization technique that controls the mean and vari-', 'ance of the learned parameters, and we show how it can be effi-', 'ciently implemented in modern deep learning frameworks. Finally,', '(iii) we demonstrate empirically, as far as we know, the lowest re-', 'ported quantization bit-widths for compressed transformer models', 'with only minor accuracy degradations measured on common bench-', 'mark tests - only 0.2% relative degradation with 3-bit precision.', '. BACKGROUND', 'Given a vector of weights w, each individual weight wi is lin-', 'early mapped to a finite range, typically [−1', '], by normalizing:', 'ˆ', 'wi = 2', 'wi−min(w)', 'max(w)−min(w) −1 where min(w) and max(w) are stored', 'separately for each weight field. To quantize the weights', 'b points', 'are spaced uniformly [−1', '] and ˆ', 'wi is mapped to the closest point:', 'Q(wi) = (round[2b−1( ˆ', 'wi + 1) −0.5] + 0.5) ∗21−b −1.', '(1)', 'With the quantization aware training approach, the quantized pa-', 'rameters are directly optimized with STE [16], since the round[⋅]', 'operation is non-differentiable.', 'Outliers in the distribution of the unquantized weights stretch', 'the weight range and wider ranges space the quantized points fur-', 'ther apart, increasing the quantization error: ∣wi −Q(wi)∣. This', 'can be mitigated by approaches that attempt to adapt to the natural', 'bell-curve of the weight distribution (discussed in Section 6). We', 'adopt a different approach, shaping the weight distribution so that it', 'resembles the uniform distribution where each quantization level is', 'utilized equally.', 'Our method can be applied to any weight fields, but in the rest', 'of this paper, we focus on the transformer neural network architec-', 'ture [18]. The model-weights of the transformer are contained in', 'affine linear operations that transform an input x by Wx + b where', 'W is a matrix of weights and b is a vector of bias terms. We focus', 'on quantizing the weight matrices W that form the bulk of the size', 'and computation in the network.', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-50']
guo22_interspeech	['The Chinese University of Hong Kong, Hong Kong SAR, China']
bae22_interspeech	['Speech AI Lab, NCSOFT Corp., Republic of Korea']
tsiamas22_interspeech	['TALP Research Center, Universitat Politecnica de Catalunya, Barcelona']
xu22b_interspeech	['Meta AI']
kim22_interspeech	['Qualcomm AI Research†, Qualcomm Korea YH, Seoul, Republic of Korea']
luo22_interspeech	['Ping An Technology (Shenzhen) Co., Ltd.', 'Aquinas International Academy, CA, USA']
munakata22_interspeech	['SANKEN, Osaka University, Osaka, Japan']
pan22_interspeech	['College of Computer Science, Inner Mongolia University, National & Local Joint Engineering', 'Research Center of Intelligent Information Processing Technology for Mongolian, Inner Mongolia', 'Key Laboratory of Mongolian Information Processing Technology', 'National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences']
fietkau22_interspeech	['Institute of Acoustics and Speech Communication, Technische Universit¨at Dresden, Germany']
chen22_interspeech	['School of Electronic and Information Engineering, South China University of Technology, China', 'UBTECH Robotics Corp, China', 'iFLYTEK Research, China']
mateju22_interspeech	['Faculty of Mechatronics, Informatics and Interdisciplinary Studies,', 'Technical University of Liberec, Studentska 2', ' 17 Liberec, Czech Republic']
liu22_interspeech	['Department of Computer Science, Indiana University, USA']
choi22_interspeech	['Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea']
zhao22_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China']
masuyama22_interspeech	['Tokyo Metropolitan University, Tokyo, Japan']
shi22_interspeech	['Toyota Technological Institute at Chicago', 'Meta AI']
kitamura22_interspeech	['Konan University, Japan', 'Wakayama University, Japan', 'Aichi Shukutoku University, Japan']
kunihara22_interspeech	['Graduate School of Engineering, The University of Tokyo', 'Faculty of Global Communication, Kobe Gakuin University']
rui22_interspeech	"['Beijing Key Laboratory of Mobile Computing', 'and Pervasive Device, Institute', 'of Computing Technology,Chinese Academy', 'of Sciences, Beijing, China', 'ABSTRACT', 'The recently proposed Mean Teacher method, which exploits', 'large-scale unlabeled data in a self-ensembling manner, has', 'achieved state-of-the-art results in several semi-supervised', 'learning benchmarks. Spurred by current achievements, this', 'paper proposes an effective Couple Learning method that', 'combines a well-trained model and a Mean Teacher model.', 'The suggested pseudo-labels generated model (PLG) in-', 'creases strongly- and weakly-labeled data to improve the', ""Mean Teacher method's performance. Moreover, the Mean"", ""Teacher's consistency cost reduces the noise impact in the"", 'pseudo-labels introduced by detection errors. The experimen-', 'tal results on Task 4 of the DCASE2020 challenge demon-', 'strate the superiority of the proposed method, achieving', 'about 44.25% F1-score on the validation set without post-', ""processing, significantly outperforming the baseline system's"", '.39%. furthermore, this paper also propose a simple and', 'effective experiment called the Variable Order Input (VOI)', 'experiment, which proves the significance of the Couple', 'Learning method. Our developed Couple Learning code is', 'available on GitHub.', 'Index Terms— semi-supervised, pseudo-label, Mean', 'Teacher, sound event detection', '. INTRODUCTION', 'Sound carries a large amount of information about our every-', 'day environment and physical events. Therefore, developing', 'signal processing methods to extract this information has', 'huge potential in several applications, such as searching for', 'multimedia based on audio content by creating context-aware', 'mobile devices, robots, and cars. Promoted by the annual', 'DCASE challenges [1', '', '], the SOTA in weakly labeled', 'semi-supervised sound event detection (SED) has progressed', 'rapidly during recent years. Indeed, several approaches have', 'been proposed for weakly labeled SED [4', '', '], such as an in-', 'tegrated pretrained deep neural network [4], classifier chains', '[5], and self-supervised auxiliary [6].', 'Most recent SOTA', 'approaches, e.g., [7', '', '', '], rely on post-processing, where', 'the neural network performs audio tagging [11', '', '', ']', 'by learning to attend to the time range where the sound', 'event is active. Teacher-student approaches dominate Semi-', 'supervised SED [15', '], where the teacher and student', 'networks are jointly trained to employ an additional loss for', 'consistency between their predictions on unlabeled data.', 'Currently, several scholars separate the use of pseudo-', 'label data from the Mean Teacher method [17], with some', 'scholars focusing on improving the pseudo-labels data on', 'semi-supervised tasks [18', '', '], while others focus on', 'improving semi-supervised tasks utilizing the Mean Teacher', 'method [21', '', ']. For example, [24] employs the audio', 'tagging system to generate pseudo labels with consistency', 'training. Unlike current trends, this paper develops an en-', 'semble method, entitled Couple Learning 1 , which utilizes', 'two deep learning models for semi-supervised sound event', 'detection (SED). The two models have the distinctive objec-', 'tives of generating pseudo-labels (PLG) from the unlabeled', 'and weakly-labeled data in the original dataset and employ-', 'ing a Mean Teacher approach to predict sound classes using', ""original data and the PLG's outputs. A thorough description"", 'of the proposed method is given in Fig. 1.', 'Our architecture involves two main stages. The first stage', 'employs a well-trained model to create pseudo weak and', 'strong labels for the original unlabeled data and pseudo strong', 'labels for the original weakly labeled data. The pseudo-labels', 'generated at this stage are quite noisy. Therefore, the second', 'stage utilizes the Mean Teacher model to exploit the baseline', 'and generate pseudo datasets. The baseline dataset contains', 'a few strongly labeled data and weakly labeled data, and a', 'large number of unlabeled data. In contrast, the generated', 'pseudo dataset contains pseudo-strong labels generated from', 'unlabeled data (UPS), pseudo-weak labels generated from', 'unlabeled data (UPW), and pseudo-strong labels generated', 'from weakly-labeled data (WPS) data.', 'code', 'is', 'available', 'at', 'https://github.com/Toshiba-China-', 'RDC/dcase20 task4', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-103']"
stan22_interspeech	['Communications Department,', 'Technical University of Cluj-Napoca, Romania']
yang22_interspeech	['Department of Electrical and Computer Engineering, National University of Singapore', 'School of Future Technology, South China University of Technology', 'The Chinese University of Hong Kong, Shenzhen, China 4Kriston AI, China']
muller22_interspeech	['Fraunhofer AISEC', 'Technical University Munich', 'why do birds GmbH']
ahn22_interspeech	['School of Electrical Engineering and Computer Science,', 'Gwangju Institute of Science and Technology, Gwangju, Korea', 'Electronics and Telecommunications Research Institute, Daejeon, Korea']
baas22_interspeech	['MediaLab, E&E Engineering, Stellenbosch University, South Africa']
pesoparada22_interspeech	['Samsung Research, UK']
shor22_interspeech	"['Verily Life Sciences1, Google Research2', 'ABSTRACT', 'Recent advances in self-supervision have dramatically im-', 'proved the quality of speech representations. However, de-', 'ployment of state-of-the-art embedding models on devices', 'has been restricted due to their limited public availability and', 'large resource footprint.', 'Our work addresses these issues', 'by publicly releasing a collection of paralinguistic speech', 'models1 that are small and near state-of-the-art performance.', 'Our approach is based on knowledge distillation, and our', 'models are distilled on public data only. We explore different', 'architectures and thoroughly evaluate our models on the Non-', 'Semantic Speech (NOSS) benchmark. Our largest distilled', 'model achieves over 96% the accuracy on 6 of 7 tasks, is', 'less than 15% the size of the original model (314MB vs', '.2GB), and is trained on 6.5% the data. The smallest model', 'achieves over 90% the accuracy on 6 of 7 tasks and is 1% in', 'size (22MB). Our models outperform the 1.2GB open source', 'Wav2Vec 2.0 model on 5 of 7 tasks despite being less than a', 'third the size, and one of our models outperforms Wav2Vec', '.0 on both emotion recognition tasks despite being less than', '% the size.', 'Index Terms— speech, representations, on-device, par-', 'alinguistic speech', '. INTRODUCTION', 'Self-supervised learning for audio has improved the qual-', 'ity of speech representations, resulting in huge performance', 'gains on downstream tasks [1', '', ']. However, a few obsta-', 'cles prevent these representations from being widely adopted', 'on devices, particularly for paralinguistic speech tasks which', 'focus on aspects of speech other than the textual meaning.', 'First, recent self-supervised models are often extremely large,', 'making them challenging to use in resource-constrained envi-', ""ronments like mobile phones (e.g. HuggingFace's Wav2Vec"", '.0 [3] at 1.2 GB). Second, due to the private nature of most', 'speech data, high-performance models are often not publicly', 'released (e.g. CAP12 [4]). In this work, we overcome both', 'constraints using public data and knowledge distillation [5].', 'Specifically, we knowledge distill a recent state-of-the-art', 'paralinguistic speech representation “Conformer Applied to', 'https://tfhub.dev/s?q=trillsson', 'Paralinguistics” (CAP12 [4]) into a series of models, which', 'we call TRILLsson.', 'Our approach primarily relies on “knowledge distilla-', 'tion” [5]. We train small student models on several fixed-', 'length input architectures, including ResNets, EfficientNets,', 'and Transformers, to match the arbitrary-length input CAP12', '(teacher) Transformer embeddings.', 'Our architectures ex-', 'plore the model size versus performance tradeoff.', 'To our', 'knowledge, this is the first successful cross-architecture dis-', 'tillation from a Transformer to non-Transformer model using', 'a different dataset.', 'We use nearly 58K hours of publicly available speech', 'data from Libri-light [6] and Audioset [7] for distillation.', 'For evaluation, we use the “Non-Semantic Speech Bench-', 'mark” (NOSS) [8]. NOSS includes 7 tasks such as emotion', 'recognition and speaker identification that require slow-time', 'features.', 'Additionally, we demonstrate the superior per-', 'formance of TRILLsson models by comparing them with', 'existing publicly available representations such as TRILL [8]', 'and Wav2Vec2.0 [3]. Our contributions are:', '. Create generally-useful paralinguistic models that are', 'small enough to run on-device.', '. Demonstrate successful cross-architecture knowledge', 'distillation from Transformers to fixed-context convo-', 'lutional networks.', '. Publicly release models at different points of the model', 'size and performance trade-off curve.', '. Identify the best paralinguistic representation in the', 'public Wav2Vec2.0 model and demonstrate that our', 'models outperform it.', '. BACKGROUND AND RELATED WORKS', 'Self-supervised representation learning has shown remark-', 'able success in vision [9] and speech recognition [3]. The', 'Wav2Vec2.0 [3] and Conformer models [10] are most rele-', 'vant to our work. [3] was one of the first frameworks to suc-', 'cessfully combine Transformers [11] and a self-supervised', 'contrastive learning objective for speech. The same training', 'objective was subsequently combined with Conformer archi-', 'tectures [10], which added convolution filters to Transformer', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-118']"
ciccarelli22_interspeech	['Amazon Alexa AI']
liu22b_interspeech	['†Institute for Infocomm Research, A*STAR, Singapore']
vuho22_interspeech	[',3 School of Information Science, Japan Advanced Institute of Science and Technology', 'Faculty of Health and Medical Sciences, Aichi Shukutoku University']
jung22_interspeech	['Naver Corporation, South Korea', 'Korea Advanced Institute of Science and Technology, South Korea']
chen22b_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'XVerse Inc., Shenzhen, China', 'Huya Inc., Guangzhou, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China']
goswami22_interspeech	['The University of Tokyo, Japan', 'RIKEN, Japan']
vaessen22_interspeech	['Institute for Computing and Information Sciences, Radboud University']
rajan22_interspeech	['College of Engineering Trivandrum, Thiruvananthapuram', 'APJ Abdul Kalam Technological University, Kerala, India']
feng22_interspeech	['Signal Analysis and Interpretation Lab (SAIL), University of Southern California']
babu22_interspeech	['△Meta AI', '□Google AI', '♢Outreach', '♣Hugging Face']
wu22_interspeech	['Georgia Institute of Technology', 'School of Electric Computer and Engineering', 'Amazon, Seattle, Washington']
gessinger22_interspeech	['Language Science and Technology, Saarland University, Saarbrucken, Germany', 'ADAPT Centre, University College Dublin, Ireland', 'Phonetics Laboratory, Linguistics, University of California, Davis, USA']
lutati22_interspeech	['Tel-Aviv University', 'Meta AI Research']
ogayo22_interspeech	['†Language Technologies Institute, Carnegie Mellon University', '‡Inspired Cognition', 'Pittsburgh, PA, USA']
chen22c_interspeech	['School of Software Engineering, South China University of Technology, Guangzhou, China']
li22b_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan', 'National Institute of Information and Communications Technology (NICT), Kyoto, Japan']
kim22b_interspeech	['School of Computer Science, University of Seoul, Republic of Korea']
pan22b_interspeech	['Institute of Data Science, National University of Singapore, Singapore', 'Key Laboratory of Cognitive Computing and Application, Tianjin University, China', 'The Chinese University of Hong Kong, Shenzhen, China', 'Machine Listening Lab, University of Bremen, Germany', 'Kriston AI, China']
li22c_interspeech	['Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of', 'Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China']
yang22b_interspeech	['Department of Electronic Engineering, Hanyang University', 'Seoul', ', Republic of Korea']
tesch22_interspeech	['Signal Processing, Universit¨at Hamburg, Germany']
yue22_interspeech	"[""Department of Engineering, King's College London, UK"", 'Speech and Hearing Research Group (SPandH), University of Sheffield, UK', 'Centre for Speech Technology Research (CSTR), University of Edinburgh, UK']"
zuo22_interspeech	['Department of Computer Science and Engineering', 'The Hong Kong University of Science and Technology']
fukuda22_interspeech	['IBM Research AI, Chuo-ku Hakozaki-cho, Tokyo, JAPAN', 'IBM Research AI, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA']
li22d_interspeech	['†Department of Computer Science and Technology, BNRist, Tsinghua University, China', '‡Tencent AI Lab, Shenzhen, China']
zhao22b_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China']
quintas22_interspeech	"['IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France', ""IUC Toulouse, CHU Toulouse, Service ORL de l'Hˆopital Larrey, Toulouse, France"", 'Laboratoire de NeuroPsychoLinguistique, UR 4156, Universite de Toulouse, Toulouse, France']"
agaskar22_interspeech	['Amazon Alexa', 'Cambridge, MA, USA']
kaland22_interspeech	['Institute of Linguistics, University of Cologne, Germany']
prananta22_interspeech	['Multimedia Computing Group, Delft University of Technology, the Netherlands', 'Netherlands Cancer Institute, Amsterdam, the Netherlands', 'ACLC, University of Amsterdam, Amsterdam, the Netherlands']
vangysel22_interspeech	['Apple']
borsos22_interspeech	['Google Research']
hou22_interspeech	['∗WAVES Research Group', 'Ghent University, Belgium', 'KU Leuven, Belgium', 'Meta AI, USA']
bernhard22_interspeech	['University of Zurich, Switzerland', 'University of Bern, Switzerland', 'University of Geneva, Switzerland']
uchida22_interspeech	['Hitachi, Ltd., Japan']
harvill22_interspeech	['University of Illinois at Urbana-Champaign, United States', 'Korea Advanced Institute of Science and Technology, South Korea']
zhan22_interspeech	['NetEase Games AI Lab, Guangzhou, China']
yang22c_interspeech	['Department of Information and Communication Engineering, Tokyo Institute of Technology,', 'Tokyo, Japan', 'Research Institute of International Chinese Language Education, Beijing Language and Culture', 'University, Beijing, China']
choi22b_interspeech	['Department of Electronic Engineering', 'Hanyang University, Seoul, Republic of Korea']
irino22_interspeech	['Faculty of Systems Engineering, Wakayama University, Japan']
chen22d_interspeech	['Yu-Chuan Chang1', 'Yen-Cheng Chang1', 'Yi-Ren Yeh2', 'E.SUN Financial Holding CO., LTD., Taiwan', 'Department of Mathematics, National Kaohsiung Normal University, Taiwan']
nguyen22_interspeech	['VinAI Research, Hanoi, Vietnam']
liang22b_interspeech	"[""CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an,"", 'China']"
yang22d_interspeech	['Speech and Audio Signal Processing Laboratory, Faculty of Information Technology,', 'Beijing University of Technology, Beijing, China']
shen22_interspeech	['The Department of Electronic Engineering and Information Science (EEIS)', 'University of Science and Technology of China, Hefei, China']
kim22c_interspeech	['Department of Electrical and Computer Engineering and INMC,', 'Seoul National University, Seoul, South Korea', 'Samsung Research, Samsung Electronics, Republic of Korea']
algayres22_interspeech	['ENS-PSL, EHESS, CNRS, Paris 2Inria, Paris', 'Facebook AI Research']
koutini22_interspeech	['Institute of Computational Perception1 & LIT AI Lab2, Johannes Kepler University Linz, Austria']
zhang22b_interspeech	['SenseTime Research']
chen22e_interspeech	['Key Laboratory of Speech Acoustics & Content Understanding, Institute of Acoustics, CAS, China', 'University of Chinese Academy of Sciences, Beijing, China']
aguirre22_interspeech	['University of Texas at El Paso, Computer Science Department, USA', 'New Mexico State University, Department of Communication Disorders, USA']
sanchez22_interspeech	['Alexa AI, Amazon']
sadeghi22_interspeech	['Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France']
sunder22_interspeech	['The Ohio State University, Columbus, OH, USA', 'IBM Research AI, Yorktown Heights, NY, USA']
wu22b_interspeech	['Georgia Institute of Technology', 'School of Electrical and Computer Engineering']
zhang22c_interspeech	['Alexa AI, Amazon']
hu22_interspeech	['Google LLC, USA']
yin22_interspeech	['University of Science and Technology of China, Hefei, China', 'Microsoft Research Asia, Beijing China', 'Microsoft Azure Speech, Beijing, China']
mehmood22_interspeech	['Samsung Research UK']
kanda22b_interspeech	['Microsoft Cloud+AI, USA', 'Microsoft Research Asia, China']
udagawa22_interspeech	['Graduate School of Information Science and Technology, The University of Tokyo, Japan.']
mitsui22_interspeech	['rinna Co., Ltd., Japan', 'Nagoya Institute of Technology, Japan']
novitasari22_interspeech	['IBM Research - Tokyo, Japan', 'Nara Institute of Science and Technology, Japan']
kishiyama22_interspeech	['Graduate School of Arts and Sciences, The University of Tokyo, Japan', 'Faculty of Foreign Studies, Nagoya Gakuin University, Japan']
zhao22c_interspeech	['Beijing National Research Center for Information Science and Technology', 'Department of Electronic Engineering, Tsinghua University, Beijing 100084, China']
saijo22_interspeech	['Waseda University, Japan', 'LINE Corporation, Japan']
luong22_interspeech	['Vinai Research', 'Deezer Research and Development']
saijo22b_interspeech	['Waseda University, Japan', 'LINE Corporation, Japan']
liu22c_interspeech	['Microsoft Azure Speech', 'Microsoft Research Asia']
horii22_interspeech	['Toyohashi University of Technology', 'Tokushima University', 'National Institute of Technology, Anan College', 'Nippon Telegraph and Telephone Corporation']
zhang22d_interspeech	['Department of Computer Science, Indiana University, USA', 'Department of Speech, Language and Hearing Sciences, Indiana University, USA', 'Department of Speech and Hearing Sciences, University of Washington, USA']
yuan22_interspeech	['VXI, China']
zheng22_interspeech	['Speech Lab, Alibaba Group']
saeki22_interspeech	['LINE Corporation, Japan. 2The University of Tokyo, Japan.']
mitsui22b_interspeech	['rinna Co., Ltd., Japan']
zhao22d_interspeech	['Microsoft Research Asia', 'University of Technology Sydney']
saeki22b_interspeech	['Graduate School of Information Science and Technology, The University of Tokyo, Japan.']
kim22d_interspeech	['Department of Artificial Intelligence, Dongguk University, Seoul, Korea']
saito22_interspeech	['The University of Tokyo, Japan', 'LINE Corp., Japan.']
koizumi22_interspeech	['Google Research', 'Tokyo University of Agriculture and Technology']
koizumi22b_interspeech	['Google Research']
yoshinaga22_interspeech	['Toyohashi University of Technology, Japan', 'National Institute for Japanese Language and Linguistics, Japan']
makishima22_interspeech	['NTT Corporation, Japan']
sanada22_interspeech	['Graduate School of Engineering, The University of Tokyo,', 'School of Computing, Tokyo Institute of Technology,', 'RIKEN AIP', ' Fujitsu']
liu22d_interspeech	['Institute of Acoustics, Chinese Academy of Sciences, Beijing, China']
park22_interspeech	['Samsung Research, Samsung Electronics, Republic of Korea', 'PDMI RAS, Russia', 'Mobile eXperience Business, Samsung Electronics, Republic of Korea']
segal22_interspeech	"['Keshet1', 'Faculty of Electrical and Computer Engineering, Technion–Israel Institute of Technology, Israel', ""Laboratoire de Sciences Cognitives et Psycholinguistique, Departement d'Etudes Cognitives, ENS,"", 'EHESS, CNRS, PSL University, France', 'Department of Linguistics, Northwestern University, IL, USA', 'Department of Communicative Sciences and Disorders, New York University, NY, USA', 'Department of Computer Science and School of Communication Sciences and Disorders,', 'University of Western Ontario, Ontario, Canada']"
puffay22_interspeech	['KU Leuven, Dept. Neurosciences, ExpORL, Leuven, Belgium', 'KU Leuven, Dept. of Electrical engineering (ESAT), PSI, Leuven, Belgium']
iwamoto22_interspeech	['Doshisha University', 'NTT Corporation']
lin22_interspeech	['Shi1, Weizhen Huang1, Yapeng Mao1', 'Zhejiang Dahua Technology CO. LTD, Hangzhou, China', 'Zhejiang Provincial Key Laboratory of Harmonized Application of Vision & Transmission,', 'Hangzhou, China']
proszewska22_interspeech	['Jagiellonian University, Poland', 'Alexa AI']
nihei22_interspeech	['NTT Corporation', 'Seikei University']
ohsugi22_interspeech	['NTT Human Informatics Laboratories, NTT Corporation']
vaaras22_interspeech	['Unit of Computing Sciences, Tampere University, Finland', 'Helsinki University Hospital, Helsinki, Finland']
rumberg22_interspeech	['Institut fur Informationsverarbeitung - L3S, Leibniz University Hannover, Germany', 'Institut fur Sonderpadagogik, Leibniz University Hannover, Germany']
rumberg22b_interspeech	['Institut fur Informationsverarbeitung - L3S, Leibniz University Hannover, Germany', 'Institut fur Sonderpadagogik, Leibniz University Hannover, Germany']
park22b_interspeech	['LINE Corp., Tokyo, Japan']
soky22_interspeech	['Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan', 'National Institute of Information and Communications Technology (NICT), Kyoto, Japan']
matsui22_interspeech	['Riken Center for Brain Science', 'Sophia University', 'Waseda University', 'Duke University']
xu22c_interspeech	['Beijing Institute of Technology, Beijing, China', 'Samsung Research China-Beijing (SRC-B)']
pelloin22_interspeech	"[""Laboratoire d'Informatique de l'Universite du Mans (LIUM), France"", 'Aix Marseille Univ, Universite de Toulon, CNRS, LIS, Marseille, France', ""Institut National de l'Audiovisuel (INA), France"", 'Naver Labs Europe (NLE), France']"
zanonboito22_interspeech	"[""Laboratoire d'Informatique d'Avignon (LIA) - Avignon University, Avignon - France"", 'NAVER LABS Europe, Meylan - France']"
liu22e_interspeech	['School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore', 'CLSP and HLT-COE, Johns Hopkins University, USA', 'Psychology, School of Social Sciences, Nanyang Technological University, Singapore']
lyu22_interspeech	['Tiansuan Lab, Security BG, Ant Group', 'Shanghai, China']
weise22_interspeech	['Heismann2, Maria Schuster3, Seung Hee Yang1', 'Speech & Language Processing Lab. Friedrich-Alexander-Universit¨at Erlangen-Nurnberg, Germany', 'Pattern Recognition Lab. Friedrich-Alexander-Universit¨at Erlangen-Nurnberg, Germany', 'Department of Otorhinolaryngology, Head and Neck Surgery. Ludwig-Maximilians University,', 'Munich, Germany']
lee22b_interspeech	['Department of Electronic Engineering Hanyang University', 'Hyundai Motor Company, Seoul, Republic of Korea']
karlapati22_interspeech	['Alexa AI, Amazon, Cambridge, United Kingdom']
neijman22_interspeech	['van Son1,3, Michiel M. W. M. van den Brekel1,3', 'Netherlands Cancer Institute, Amsterdam, The Netherlands', 'Radboud Universiteit, Nijmegen, The Netherlands', 'ACLC, University of Amsterdam, Amsterdam, The Netherlands']
deseyssel22_interspeech	['Cognitive Machine Learning (ENS–CNRS–EHESS–INRIA–PSL Research University), France', 'Universite de Paris Cite, CNRS, Laboratoire de linguistique formelle, F-75013 Paris, France', 'Meta AI Research, France']
salim22_interspeech	[',3Dept of Electronics and Communication Engineering, NIT Calicut, India', 'Dept of Electronics and Communication Engineering, NIT Patna, India']
fernau22_interspeech	['† Technische Universitat Berlin', '⋆German Center For Artificial Intelligence (DFKI)']
he22_interspeech	['Department of Computer Science', 'University of Oxford, Oxford, UK']
makarov22_interspeech	['Alexa AI, Amazon']
lee22c_interspeech	['Graduate School of Artificial Intelligence, POSTECH, Republic of Korea', 'Department of Computer Science and Engineering, POSTECH, Republic of Korea']
pupier22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, France']
ammarabbas22_interspeech	['Alexa AI, Amazon']
cattan22_interspeech	['Universite Paris-Saclay, CNRS, LISN', ' Orsay, France', 'QWANT', ' boulevard Haussmann', ' Paris, France']
fernandez22_interspeech	['IBM Research AI']
pundak22_interspeech	['Google Inc., U.S.A']
soltau22_interspeech	['Google Brain']
tsukada22_interspeech	['Macquarie University, Australia', 'The University of Melbourne, Australia', 'Inner Mongolia University, China']
lu22_interspeech	['Google Inc.']
olivier22_interspeech	['Language Technologies Institute, Carnegie Mellon University, USA']
janbakhshi22_interspeech	['Idiap Research Institute, Martigny, Switzerland', 'Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland']
nishimura22_interspeech	['The University of Tokyo, Japan', 'LINE Corp., Japan.']
manocha22_interspeech	['Department of Computer Science, Princeton University, USA', 'Adobe Research, USA']
manocha22b_interspeech	['Department of Computer Science, Princeton University, Princeton, NJ, USA', 'Meta Reality Labs Research, Redmond, WA, USA']
manocha22c_interspeech	['Department of Computer Science, Princeton University, Princeton, NJ, USA', 'Meta Reality Labs Research, Redmond, WA, USA']
bekal22_interspeech	['AWS AI Labs']
seshadri22_interspeech	['Apple']
fu22_interspeech	['JD AI Research, Beijing, China']
fasoli22_interspeech	['IBM Research, USA']
shi22b_interspeech	['Language Technologies Institute, Carnegie Mellon University, PA, USA', 'IBM Research AI, Yorktown Heights, NY, USA']
he22b_interspeech	['Frank K. Soong2', 'The Hong Kong University of Science and Technology 2 Microsoft China']
mussakhojayeva22_interspeech	['Institute of Smart Systems and Artificial Intelligence (ISSAI),', 'Nazarbayev University, Nur-Sultan, Kazakhstan']
shim22_interspeech	['Department of Electrical and Computer Engineering, Seoul National University, Korea']
lou22_interspeech	['Hikvision Research Institute, Hangzhou, China']
cheng22_interspeech	['School of Information Science and Engineering, Southeast University, China', 'School of Information and Communication Engineering, Nanjing Institute of Technology, China', 'Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany', 'GLAM – Group on Language, Audio, & Music, Imperial College London, UK', 'vivo Mobile Commun co Ltd, China']
pia22_interspeech	['Fraunhofer IIS Erlangen']
yang22e_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China', 'Center for Vision, Speech and Signal Processing, University of Surrey, UK']
song22_interspeech	"['Microsoft Research', ""Department of Oral and Craniomaxillofacial Surgery, Shanghai Ninth People's Hospital,"", 'Shanghai Jiao Tong University School of Medicine']"
saeki22c_interspeech	['The University of Tokyo, Japan.']
kunihara22b_interspeech	['Graduate School of Engineering, The University of Tokyo', 'Faculty of Global Communication, Kobe Gakuin University']
li22e_interspeech	['National Institute of Informatics, Japan', 'The Graduate University for Advanced Studies (SOKENDAI), Japan']
ho22_interspeech	['† Japan Advanced Institute of Science and Technology, Japan']
takamichi22_interspeech	['The University of Tokyo, Japan']
zhou22_interspeech	['Speech and Audio Signal Processing Laboratory, Faculty of Information Technology, Beijing University', 'of Technology, Beijing 100124, China.', 'ABSTRACT', 'In this paper, a new multi-source wideband direction of arrival', '(MSW-DOA) estimation method is proposed for the signals with', 'non-uniform distribution using the sub-array of uniform linear', 'array. Different from conventional methods, based on the free far-', 'field model, the proposed method mainly makes two contributions.', 'One is that the sub-array decomposition is adopted to improve the', 'accuracy of MSW-DOA estimation by minimizing the weighted', 'error, and the other one is that the frequency focusing procedure is', 'optimized according to the presence probability of sound sources', 'for reducing the influence of the sub-bands with low signal to', 'noise ratio (SNR). Simulation results show that the proposed', 'method can effectively improve the performance of wideband', 'DOA estimation in the case of multiple sound sources.', 'Index Terms: microphone array, direction of arrival, frequency', 'focusing, subspace decomposition', '. Introduction', 'Multi-source wideband direction of arrival (MSW-DOA)', 'estimation is a research hotspot in the fields of radar, sonar,', 'wireless communication, speech signal processing, etc. [1-10]. An', 'accurate direction of arrival (DOA) estimation method is a', 'prerequisite for the subsequent processing and applications, such', 'as in beamforming', '[6]. The conventional wideband DOA', 'estimation methods are based on the Fourier transform [11-13],', 'which transform the signal from time domain to frequency domain,', 'and further design the wideband DOA estimation methods', 'according to the narrowband DOA estimation methods [9,11-13]. For', 'example, the incoherent signal subspace (ISS) method proposed', 'by Su [14] used the mean of the estimated DOAs of all sub-bands', 'as the result of wideband DOA estimation. However, the signal', 'may not be an uniform distribution within the bandwidth, which', 'may cause a great error of DOA estimation in the sub-band with', 'lower signal-to-noise ratio (SNR), and ISS method is not suitable', 'for the coherent signals processing [8,9]. Therefore, Wang [15]', 'proposed the coherent signal subspace (CSS) method by', 'frequency focusing, which can reduce the coherence by smoothing', 'within sub-bands [16,17], and make the covariance matrix with full', 'rank [7-9].', 'The core of CSS method is the procedure of frequency focus-', 'ing [9,17]. Conventional frequency focusing methods include signal', 'subspace transformation (SST) [18], rotational signal sub-space', '(RSS) [16], two-sided correlation transformation (TCT) [19], etc.', 'Those methods are based on the premise that the focusing matrix', 'is an unitary matrix and the noise is Gaussian white noise [5,9]. In', 'addition, most focusing methods need to estimate a preliminary', 'DOA, which may cause a large error when the preliminary DOA', 'is inaccuracy [2~9]. Thus, Ma [8] proposed the focusing signal', 'subspace (FSS) method, which mainly focused on the procedure', 'without the estimation of preliminary DOA [4,9,11,12]. Although', 'FSS method seems very effective to MSW-DOA estimation, there', 'are still many drawbacks in practical application, such as the noise', 'may not be Gaussian white noise, source signals are non-uniform', 'distribution within bandwidth, the signal subspace is very', 'sensitive to the error of focusing model, etc [3,4,8,11]. Therefore, to', 'overcome the above limitations, a new FSS based method is', 'proposed in this paper for improving the performance of MSW-', 'DOA estimation.', '. Signal model and problem description', 'Considering an uniform linear array (ULA) composed of M+B', 'microphones, the distance between the microphones is d. Based', 'on the free far-field model, the ULA can be divided into B+1', 'successive sub-arrays, and the microphone number of each sub-', 'array is M, as shown in figure 1. Based on this particular division,', 'we can ensure same performance for each sub-array, and the DOA', 'of each sound source can be regarded as the same for each sub-', 'array. Assuming there are Q sound sources contained in acoustic', 'field, the time domain model of the output observed signal xb(t) of', 'the bth sub-array can be expressed as follows [4,8]:', '( )', '( )', '( )', ',', 'Q', 'b', 'b q', 'b', 'q', 't', 't', 't', '=', '=', '+', '\uf0e5', 'x', 's', 'n', '(1)', 'where sb,q(t) and nb(t) are the qth sound source and noise related to', 'the bth sub-array at time t, respectively. b=1,2,…,B+1.', 'mic 1', 'mic 2', 'mic 3', 'mic M', 'mic M+1', '...', 'Sub-array 1', 'mic M+B', '...', 'Sub-array 2', 'Sub-array B+1', '...', '...', '...', 'd', 'θq', 'Figure 1: Uniform linear array and sub-arrays', 'Taking the center microphone of each sub-array as the', 'reference microphone, and performing K-point short time Fourier', 'transform (STFT) on Eq. (1), the observed signal at the kth sub-', 'band of the lth frame is obtained as follows:', '(', ')', '(', ')', '(', ')', '(', ')', ',', ',', ',', ',', 'Q', 'b', 'k', 'b', 'k', 'q', 'q', 'k', 'b', 'k', 'q', 'f l', 'f', 'S', 'f l', 'f l', '\uf071', '=', '=', '+', '\uf0e5', 'X', 'a', 'N', '(2)', 'where Xb(fk,l), Sq(fk,l) and Nb(fk,l) are the STFT results of xb(t), sq(t)', 'and nb(t), respectively. sq(t) is the signal of the qth sound source. fk', 'is the frequency of the kth sub-band, θq is the incident angle of the', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-445']
zhao22e_interspeech	['School of Electronic Information and Electrical Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
taniguchi22_interspeech	['Doshisha University, Japan', 'MINDWORD Co. Ltd., Japan']
zhang22e_interspeech	['Tencent Inc, Guangzhou, China']
sun22_interspeech	['Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.']
lesnichaia22_interspeech	['Peter the Great St. Petersburg Polytechnic University, St. Petersburg, Russia', 'The University of Aizu, Aizu-Wakamatsu, Japan', 'Speech Technology Center, St. Petersburg, Russia']
xiong22_interspeech	['Hummingbird Audio Lab, Alibaba Group, Hangzhou, China', 'Westlake University & Westlake Institute for Advanced Study, Hangzhou, China']
zhang22f_interspeech	['Elevoc Technology Co., Ltd, Shenzhen, China']
sun22b_interspeech	['Department of Machine Intelligence, Speech and Hearing Research Center, and Key Laboratory of', 'Machine Perception (Ministry of Education), Peking University, Beijing, China']
liu22f_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
liu22g_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
zhang22g_interspeech	"['Xie2,∗, Chao Yang1,3, Fuping Pan1, Jianwei Niu1', 'Horizon Robotics, Beijing, China', ""Northwestern Polytechnical University, Xi'an, China"", 'WeNet Open Source Community']"
liu22h_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
du22b_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
hughes22_interspeech	['Department of Language and Linguistic Science, University of York, UK']
xiong22b_interspeech	['Department of Electronic Engineering and Information Science,', 'University of Science and Technology of China, Hefei, China', 'Hummingbird Audio Lab, Alibaba Group, Hangzhou, China']
zhao22f_interspeech	['Gupta2, Ming Zhao1', 'Arizona State University', 'Amazon Alexa Speech']
wu22c_interspeech	['Philips Research, Eindhoven, The Netherlands', 'University of Cagliari, Cagliari, Italy']
wang22c_interspeech	['Institute of Information Science, Academia Sinica', 'Research Center for Information Technology Innovation, Academia Sinica']
azeemi22_interspeech	['Lahore University of Management Sciences']
cao22_interspeech	['University of Stuttgart, Institute of Signal Processing and System Theory, Stuttgart, Germany']
wang22d_interspeech	['Department of Language and Linguistic Science, University of York, UK']
huang22b_interspeech	['Graduate Institute of Computer Science and Information Engineering, National Taiwan University', 'Department of Physics, National Taiwan University', 'Google Brain', 'Graduate Institute of Communication Engineering, National Taiwan University']
wu22d_interspeech	['Music and Audio Research Laboratory, New York University, USA', 'Descript, Inc.']
berg22_interspeech	['Arm, Sweden', 'Lund University, Sweden', 'Tenstorrent, Germany']
wang22e_interspeech	['School of Mathematical and Computational Sciences, Massey University, New Zealand']
lee22d_interspeech	['Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan', 'Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan', 'Institute of Information Science, Academia Sinica, Taipei, Taiwan']
ghosh22_interspeech	['M Health Information Systems']
yeh22_interspeech	['School of Informatics, University of Edinburgh']
qian22_interspeech	['School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China']
liu22i_interspeech	['Inner Mongolia University, China 2 The Chinese University of Hong Kong, Shenzhen, China', 'National University of Singapore 4 Singapore University of Technology and Design', 'Imperial College London, United Kingdom']
nie22_interspeech	['DiDi Chuxing, Beijing, China']
zhu22_interspeech	['Department of Linguistics, University of Michigan, Ann Arbor, USA', 'Center for Language Studies, Radboud University, Nijmegen, Netherlands', 'School of Information, University of Michigan, Ann Arbor, USA']
kim22e_interspeech	['School of Industrial Management Engineering, Korea University, Seoul, Korea']
lee22e_interspeech	['Kakao Enterprise Corp., South Korea']
wang22f_interspeech	['National Laboratory of Pattern Recognition, Institute of Automation, CAS', 'School of Artificial Intelligence, University of Chinese Academy of Sciences']
dutta22_interspeech	"['and John H.L. Hansen1*', 'Center for Robust Speech Systems, The University of Texas at Dallas, Richardson, Texas, USA', ""Juniper Gardens Children's Project, The University of Kansas, Kansas City, Kansas, USA""]"
li22g_interspeech	"['State Key Laboratory for Novel Software Technology, Nanjing University', ""Huawei Noah's Ark Lab""]"
song22b_interspeech	['School of Digital Humanities and Social Sciences, Korea Advanced Institute of Science and', 'Technology, South Korea', 'School of Humanities, Language and Global Studies, University of Central Lancashire, UK', 'Oriental Institute, University of Oxford, UK']
zhang22h_interspeech	['Shenzhen International Graduate School, Tsinghua University', 'TEG AI, Tencent Inc', 'Graduate Institute of Communication Engineering, National Taiwan University', 'Centre for Perceptual and Interactive Intelligence, The Chinese University of Hong Kong']
kawano22_interspeech	['Guardian Robot Project, RIKEN, Japan', 'Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan', 'School of Informatics, Kyoto University, Japan']
chang22_interspeech	['Google Inc., U.S.A']
gupta22_interspeech	['Department of Computer Science and Engineering, Indian Institute of Technology, Indore, India', 'School of Computer Science and Engineering, Nanyang Technological University, Singapore']
chang22b_interspeech	['Google Inc., U.S.A']
xue22_interspeech	"[""Northwestern Polytechnical University, Xi'an, China"", 'Tencent AI Lab, China']"
yang22f_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Ping An Technology (Shenzhen) Co., Ltd., China', 'The Chinese University of Hong Kong, Hong Kong SAR, China']
jain22_interspeech	['Indian Institute of Technology Bombay', 'Verisk Analytics']
wang22g_interspeech	['Institute of Microelectronics of the Chinese Academy of Sciences (the IME of CAS), China', 'Nanjing Institute of Intelligence Technology, the IME of CAS, China', 'University of Chinese Academy of Sciences, China']
mathur22_interspeech	['Morariu2, Rajiv Jain2, Dinesh Manocha1', 'University of Maryland, College Park', 'Adobe Research']
takeda22_interspeech	['SANKEN, Osaka University, Japan', 'Honda Research Institute Japan, Co., Ltd., Japan 3Tokyo Institute of Technology, Japan']
huang22c_interspeech	['Bigo Technology PTE. LTD, Singapore']
shin22_interspeech	['Naver Corporation, South Korea', 'Dept. of Electrical and Electronic Engineering, Yonsei University, South Korea']
kim22f_interspeech	['SK Telecom, Seoul, Republic of Korea', 'Kakao Enterprise Corporation, Seongnam, Republic of Korea']
zellers22_interspeech	['ISFAS, Kiel University, Germany']
santoso22_interspeech	['University of Tsukuba, Japan', 'RevComm, Inc., Japan', 'Waseda University, Japan']
noguchi22_interspeech	['Tokyo Medical and Dental University, Tokyo, Japan', 'Sophia University, Tokyo, Japan', 'The University of Tokyo, Tokyo, Japan', 'Nagoya Gakuin University, Aichi, Japan', 'Tokyo Kasei Gakuin University, Tokyo, Japan', 'Maebashi Institute of Technology, Gunma, Japan', 'National Institute for Japanese Language and Linguistics, Tokyo, Japan']
hu22b_interspeech	['National Engineering Research Center of Speech and Language Information Processing,', 'University of Science and Technology of China, Hefei, China.', 'ICT Cluster, Singapore Institute of Technology, Singapore.', 'iFLYTEK Research, iFLYTEK CO. LTD., Hefei, China.']
zhao22g_interspeech	['Department of Data Science & AI, Monash University']
lin22b_interspeech	['National Taiwan University, Taiwan', 'Amazon AI, USA']
heo22_interspeech	['School of Computer Science, University of Seoul, Republic of Korea']
shchekotov22_interspeech	['∗Equal contribution', 'Samsung AI Center, Moscow', 'Higher School of Economics, Moscow', 'Skolkovo Institute of Science and Technology, Moscow', 'Artificial Intelligence Research Institute, Moscow']
liu22j_interspeech	['ByteDance AI LAB', 'South China University of Technology, Guangzhou, China']
dinkel22_interspeech	['Xiaomi Corperation, Beijing, China']
li22h_interspeech	"[""Northwestern Polytechnical University, Xi'an, China"", ""School of Software Engineering, Xi'an Jiaotong University, Xi'an, China"", 'Mobvoi AI Lab, Suzhou, China']"
lin22c_interspeech	['Dong3, Shang-Wen Li3, Abdelrahman Mohamed3, Hung-yi Lee1, Lin-shan Lee1', 'National Taiwan University, Taiwan', 'Massachusetts Institute of Technology, USA', 'Meta AI, USA']
wang22h_interspeech	['Changhong AI Lab (CHAIR), Sichuan Changhong Electronics Holding Group Co., Ltd.', 'Electrical Engineering and Computer Science, Northwestern University, US']
wei22_interspeech	['Department of Computer Science, Inner Mongolia University 2Department of Electrical and', 'Electronic Engineering, Southern University of Science and Technology']
zhang22i_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong', 'Microsoft Research Asia 3 Tsinghua University 4 Microsoft Azure Speech 5 Zhejiang University']
meng22b_interspeech	['Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of', 'Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China']
abdullah22_interspeech	['Language Science and Technology (LST), Saarland University, Germany', 'Saarland Informatics Campus, Germany']
gong22_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
song22c_interspeech	['Department of Electrical and Computer Engineering, National University of Singapore, Singapore', 'South China University of Technology, China', 'The Chinese University of Hong Kong, Shenzhen, China', 'Kriston AI, China']
hu22c_interspeech	['Key Laboratory of signal detection and processing in Xinjiang, China', 'School of Information Science and Engineering, Xinjiang University, Urumqi, China', 'Department of Electronic Engineering, Tsinghua University, China']
nakata22_interspeech	['The University of Tokyo', 'Nippon Telegraph and Telephone Corporation']
priyasad22_interspeech	['Fernando1, Simon Denman1, Clinton Fookes1, Jia Tang3, David Kaye3', 'SAIVT, Queensland University of Technology, Brisbane, Australia', 'KeyLead Health, Melbourne, Australia', 'Alfred Hospital, Melbourne, Australia']
guan22_interspeech	['School of Electronics and Communication Engineering, Guangzhou University, Guangzhou, China', 'Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of', 'Sciences, Beijing, China', 'Communication University of China, Beijing, China']
zhang22j_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
qin22_interspeech	['School of Computer Science, Wuhan University, Wuhan, China', 'Data Science Research Center, Duke Kunshan University, Kunshan, China', 'Tencent AI Lab, Shenzhen, China']
zhong22_interspeech	['University of Science and Technology of China, Hefei, Anhui, P. R. China', 'iFlytek Research, Hefei, Anhui, P. R. China']
zhang22k_interspeech	['State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China']
quan22_interspeech	['PingAn Technology']
choi22c_interspeech	['Korea University, Republic of Korea', 'Chung-Ang University, Republic of Korea']
dong22_interspeech	['Peking University, China', 'Kuaishou Technology Co., Beijing, China']
sun22c_interspeech	['Department of Machine Intelligence, Speech and Hearing Research Center, and Key Laboratory of', 'Machine Perception (Ministry of Education), Peking University, Beijing, China']
yang22g_interspeech	['Artificial Intelligence Application Research Center, Huawei Technologies']
sriram22_interspeech	['Facebook AI']
luo22c_interspeech	['Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of', 'Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China']
cheng22b_interspeech	['Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of', 'Sciences, Beijing, China', 'Science & Technology on Integrated Infomation System Laboratory, Institute of Software Chinese', 'Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China']
ivry22_interspeech	['Andrew and Erna Viterbi Faculty of Electrical and Computer Engineering', 'Technion – Israel Institute of Technology, Technion City, Haifa 3200003, Israel']
wang22i_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China', 'Tencent AI Lab, Shenzhen, China']
wang22j_interspeech	['Department of Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA', 'Data Science Research Center, Duke Kunshan University, Kunshan 215316, PR China', 'AI Lab, Lenovo Research, Beijing 100085, PR China']
xu22e_interspeech	['The Chinese University of Hong Kong, Hong Kong SAR, China']
zhao22h_interspeech	['Key Laboratory of Child Development and Learning Science of Ministry of Education, Southeast', 'University, China', 'School of Information Science and Engineering, Southeast University, Nanjing 210096, China', 'School of Electronics and Information Engineering, Nanjing University of information Science and', 'Technology, China', 'Changwang School of Honors, Nanjing University of information Science and Technology, China']
deng22_interspeech	['The Chinese University of Hong Kong, Hong Kong SAR, China', 'Institute of Software, Chinese Academy of Sciences, China']
lei22_interspeech	"[""Northwestern Polytechnical University, Xi'an, China"", 'Tencent AI Lab, China']"
wu22e_interspeech	['Gaoling School of Artificial Intelligence, Renmin University of China, China', 'Microsoft Azure Speech, China', 'Universite de Montreal, China']
berisha22_interspeech	['Arizona State University', 'Aural Analytics']
tal22_interspeech	['Hebrew University of Jerusalem', 'Meta AI Research']
cui22_interspeech	['Geng1, Boyang Xue1, Xunying Liu1, Helen Meng1', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'Institute of Software, Chinese Academy of Sciences, China']
hou22b_interspeech	['Speech & Audio Team, AI Lab, ByteDance Inc., Beijing, China']
zhuang22_interspeech	['Harbin Institute of Technology, Shenzhen, China']
rixen22_interspeech	['Department of Computer Science, Kiel University, Germany']
wei22b_interspeech	"[""Xi'an Jiaotong University"", 'Nanyang Technological University', ""Xi'an University of Posts and Telecommunications""]"
bai22_interspeech	['Kuaishou Technology Co., Ltd, Beijing, China']
wang22k_interspeech	['The Chinese University of Hong Kong, Hong Kong SAR, China']
choi22d_interspeech	['Sogang University, Seoul, Republic of Korea']
ye22_interspeech	['Sanli Tian1,2,Pengyuan Zhang1,2, Yonghong Yan†1,2', 'Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, CAS, China', 'University of Chinese Academy of Sciences, China']
zhu22b_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics CAS, China', 'University of Chinese Academy of Sciences, China', 'Microsoft Research Asia, China']
lee22f_interspeech	['The Chinese University of Hong Kong, Hong Kong']
wang22l_interspeech	['The Chinese University of Hong Kong, Hong Kong SAR, China']
huang22d_interspeech	"[""School of Computer Science and Technology, Xi'an Jiaotong University"", 'Media Technology Institute, Huawei Technologies Co., Ltd.']"
yang22h_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese', 'Academy of Sciences, China', 'University of Chinese Academy of Sciences, China', 'Magic Data Technology Co., Ltd., China', 'MoE Key Lab of Artificial Intelligence and AI Institute, Shanghai Jiao Tong University, China', 'Northwestern Polytechnical University, China']
brummer22_interspeech	['Phonexia, South Africa', 'Omilia - Conversational Intelligence, Athens, Greece']
weninger22_interspeech	['Nuance Communications, Inc.']
baskar22_interspeech	['Φ Brno University of Technology, † Google Inc.,']
zhao22i_interspeech	['The University of Hong Kong, Hong Kong SAR, China', 'IIIS, Tsinghua University, Beijing, China', 'Shanghai Qi Zhi Institute, Shanghai, China']
zhang22l_interspeech	['Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou,', 'China']
lei22b_interspeech	['East China University of Science and Technology, Shanghai, China']
han22b_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
sakuma22_interspeech	['Waseda University, Japan', 'Chiba Institute of Technology, Japan']
liu22k_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustic, Chinese', 'Academy of Sciences, China', 'University of Chinese Academy of Sciences, China']
shrem22_interspeech	['Facullty of Electrical and Computer Engineering, Technion–Israel Institute of Technology, Israel', 'Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel']
maouche22_interspeech	['Tommasi1, Emmanuel Vincent2', 'Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France', 'Universite de Lorraine, CNRS, Inria, LORIA, France']
baruah22_interspeech	['Institute for Intelligent Systems, and Department of Electrical & Computer Engineering', 'University of Memphis, Memphis, TN 38152, USA']
lu22b_interspeech	['Kuaishou Technology Co., Beijing, China']
lenglet22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, France']
avila22_interspeech	"[""Huawei Noah's Ark Lab"", 'Montreal, QC H3N 1X9, Canada']"
hu22d_interspeech	['Key Laboratory of signal detection and processing in Xinjiang, China', 'School of Information Science and Engineering, Xinjiang University, Urumqi, China', 'Department of Electronic Engineering, Tsinghua University, China']
zevallos22_interspeech	['Universitat Pompeu Fabra (UPF)', 'Centre de Llenguatge i Computacio (CLiC), Universitat de Barcelona (UB)', 'Institut de Recerca en Sistemes Complexos (UBICS), Universitat de Barcelona (UB)', 'Telefonica I+D, Research']
tian22_interspeech	['Key Lab of Speech Acoustics and Content Understanding, Institute of Acoustics, CAS, China', 'University of Chinese Academy of Sciences, China']
gabeur22_interspeech	['Inria†', 'Google Research']
dissen22_interspeech	['Facullty of Electrical and Computer Engineering, Technion–Israel Institute of Technology, Israel', 'Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel']
guo22c_interspeech	['Department of Electronic Engineering and Information Science, University of Science', 'and Technology of China, Hefei, Anhui 230027, China', 'National Engineering Laboratory for Speech and Language Information Processing,', 'Hefei, Anhui 230027, China']
xue22b_interspeech	['Microsoft Research Asia, Beijing, China', 'Communication University of China, Beijing, China']
chang22c_interspeech	['National Taiwan University, Taipei, Taiwan']
zhou22b_interspeech	['Zhejiang University, Hangzhou, China', 'Peking University, Shenzhen, China']
lee22g_interspeech	['Naver Corporation', 'Carnegie Mellon University']
jeon22_interspeech	['School of Humanities, Language and Global Studies, University of Central Lancashire, UK', 'Faculty of Linguistics, Philology and Phonetics, University of Oxford, UK', 'Department of Language and Linguistics, University of Essex, UK']
johnson22_interspeech	['UCLA, Department of Electrical and Computer Engineering', 'University of Washington, Department of Electrical and Computer Engineering', 'UCLA, Department of Linguistics']
chinen22_interspeech	['Google', 'University College Dublin, School of Computer Science, Dublin, Ireland']
kawahara22_interspeech	['Wakayama University, Wakayama', '-8510 Japan', 'Tokyo University of Agriculture and Technology, Tokyo', '-8588 Japan', 'Health Science University of Hokkaido, Hokkaido', '-0293 Japan', 'Konan University, Kobe 658-8510 Japan', 'Meijo University, Nagoya 468-8502 Japan', 'Meiji University, Tokyo', '-8525 Japan']
lepage22_interspeech	"['Speaker and Language Recognition Group (ESLR),', ""Laboratoire de Recherche de l'EPITA, France""]"
jia22_interspeech	['Meta AI']
jabeen22_interspeech	['Phonetics Workgroup, Center for Cognitive Interactive Technology (CITEC)', 'Faculty of Linguistics and Literary Studies, Bielefeld University, Germany']
zhang22m_interspeech	['Department of Computer Science and Engineering, The Ohio State University, USA', 'Center for Cognitive and Brain Sciences, The Ohio State University, USA']
flemotomos22_interspeech	['Signal Analysis and Interpretation Laboratory', 'University of Southern California, Los Angeles, CA, USA']
ni22_interspeech	['Hasegawa-Johnson1', 'University of Illinois at Urbana-Champaign', 'MIT-IBM Watson AI Lab 3University of California, Santa Barbara']
rugayan22_interspeech	['Department of Electronic Systems, NTNU']
kopuklu22_interspeech	['Microsoft', 'Munich, Germany']
cui22b_interspeech	['Kingsbury1, Gakuto Kurata2', 'IBM Research AI, IBM T. J. Watson Research Center, USA', 'IBM Research Tokyo, Japan']
dingliwal22_interspeech	['ABSTRACT', 'Automatic Speech Recognition (ASR) systems have', 'found their use in numerous industrial applications in very', 'diverse domains creating a need to adapt to new domains', 'with small memory and deployment overhead. In this work,', 'we introduce domain-prompts, a methodology that involves', 'training a small number of domain embedding parameters to', 'prime a Transformer-based Language Model (LM) to a par-', 'ticular domain. Using this domain-adapted LM for rescoring', 'ASR hypotheses can achieve 7-13% WER reduction for a', 'new domain with just 1000 unlabeled textual domain-specific', 'sentences.', 'This improvement is comparable or even bet-', 'ter than fully fine-tuned models even though just 0.02% of', 'the parameters of the base LM are updated. Additionally,', 'our method is deployment-friendly as the learnt domain em-', 'beddings are prefixed to the input to the model rather than', 'changing the base model architecture. Therefore, our method', 'is an ideal choice for on-the-fly adaptation of LMs used in', 'ASR systems to progressively scale it to new domains.', 'Index Terms— domain-adaptation, prompt-tuning, gpt2,', 'multi-domain ASR, parameter-efficiency, low-data setting', '. INTRODUCTION', 'Automatic Speech Recognition (ASR) systems form a key', 'component of various products across industry. With recent', 'advancements [1', '', '], they have been deployed in a wide', 'range of domains, including healthcare, travel reservations,', 'and customer services etc. A typical technique to improve', 'the performance of these systems is to do a rescoring of the', 'n-best hypotheses with an external Language Model (LM)', '[2]. Recent pretrained Transformer-based LMs such as GPT-', '[4] and BERT [5] have been shown [6] to be more effective', 'than conventional LSTM based LMs for rescoring. However,', 'their use in an industrial ASR system that needs to incremen-', 'tally support new domains poses the following challenge. As', 'showcased in [7', '], domain-specific data is useful for im-', 'proving performance in a domain. However, retraining or', 'maintaining copies of Transformer-based LMs for each do-', 'main separately is not scalable as updating and storing mil-', 'lions of parameters comes with a large cost. Therefore, a', '*work carried out while working at Amazon', 'need for an efficient domain-adaptation method for such LMs', 'is evident. [9', '', '] used external knowledge, memory and', 'context respectively to improve performance in specific dif-', 'ficult domains, while [12', '] adapted the neural LM used', 'within the system. However, to the best of our knowledge,', 'ours is the first work to propose and study methods to do effi-', 'cient domain-adaptation of Transformer-based LMs to benefit', 'ASR systems. Language modeling literature [14', '', '] in-', 'troduced novel methodologies to solve a related problem of', 'efficiently adapting such LMs to specific tasks. Instead of', 'fine-tuning and storing millions of parameters for each task,', 'they propose augmenting the frozen task-agnostic model with', 'a handful of task-specific trainable parameters. For exam-', 'ple, AdapterHub [14] introduced new task-specific layers in', 'conjunction to frozen pre-trained weights of LMs. More re-', 'cent models, such as GPT-3 [17], are able to solve new tasks', 'with the help of just the textual descriptions of the task (called', 'prompts).', 'Extending [18], the focus of this work is to adapt such', 'LMs to different domains of the same task rather than solv-', 'ing multiple tasks. Our objective is to learn a small set of', 'domain-specific parameters to score ASR hypotheses better', 'than the base Transformer-based LM without the domain', 'data. Drawing ideas from prompt-tuning [15] for task adap-', 'tation, we introduce domain-prompts for our goal. We define', 'domain-prompts as domain-specific embeddings, which when', 'prefixed to the sequence of token embeddings, and passed', 'through a pretrained Transformer LM, return the probability', 'distribution of the next token, close to that given by a fully', 'domain-adapted LM. Our main contributions are summarized', 'as follows: (1) we introduce a new methodology domain-', 'prompts, which is the first attempt to apply prompt-tuning for', 'parameter-efficient domain-adaptation of Transformer-based', 'LMs for their use in ASR systems, (2) In new domains with', 'limited data, we demonstrate that rescoring ASR hypotheses', 'with LM adapted using our method can achieve 7-13% WER', 'reduction while using a handful of additional domain-specific', 'parameters (3) Along with saving memory and training cost,', 'domain-prompts can match or even beat the performance of', 'fully fine-tuned models with no change to the deployment', 'of the base model, thereby making it the ideal choice for', 'on-the-fly domain adaptation for industrial ASR systems.', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-824']
raitio22_interspeech	['Apple']
zhou22c_interspeech	['Human Language Technology and Pattern Recognition, Computer Science Department,', 'RWTH Aachen University', ' Aachen, Germany', 'AppTek GmbH', ' Aachen, Germany']
peng22_interspeech	['National Engineering Research Center of Speech and Language Information Processing,', 'University of Science and Technology of China, Hefei, China']
zhang22n_interspeech	['Multimedia Computing Group, Delft University of Technology, Delft, the Netherlands', 'Netherlands Cancer Institute, Amsterdam, the Netherlands', 'ACLC, University of Amsterdam, Amsterdam, the Netherlands']
lonergan22_interspeech	['Phonetics and Speech Laboratory, School of Linguistic, Speech and Communication Sciences,', 'Trinity College Dublin, Ireland', 'Engineering Department, Cambridge University, UK']
heo22b_interspeech	['†Graduate School of Artificial Intelligence,', '‡Department of Computer Science and Engineering,', 'Pohang University of Science and Technology (POSTECH), Republic of Korea']
koppelmann22_interspeech	['and Rainer Martin¹', '¹ Institute of Communication Acoustics, Ruhr-Universit¨at Bochum, Bochum, Germany', '² CISPA Helmholtz Center for Information Security, Saarbrucken, Germany']
daoudi22_interspeech	['INRIA Bordeaux Sud-Ouest, GeoStat team, France', 'ENT Department, University Hospital of Bordeaux, France', 'Department of Neurology for Neurodegenerative diseases, University Hospital of Bordeaux', 'and University of Bordeaux, CNRS, IMN, UMR 5293, France', 'MSA Reference Center, Clinical Investigation Center CIC1436,', 'Dep. Neurosciences & Clinical Pharmacology, University Hospital of Toulouse, France', 'Voice and deglutition unit , ENT department, University Hospital of Toulouse, France.']
bergler22_interspeech	['Friedrich-Alexander-University Erlangen-Nurnberg, Pattern Recognition Lab, Erlangen, Germany']
yang22i_interspeech	['National Engineering Research Center for Multimedia Software, School of Computer Science,', 'Wuhan University, Wuhan, China', 'Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan, China']
ding22_interspeech	['Google LLC, USA']
zhang22o_interspeech	['Anhui Province Key Laboratory of Medical Physics and Technology, Institute of Health and', 'Medical Technology, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei,', 'China', 'University of Science and Technology of China, Hefei, China', 'Hefei Cancer Hospital, Chinese Academy of Sciences, Hefei, China', 'Institute of Neurology, Anhui University of Chinese Medicine, Hefei, China']
kim22g_interspeech	"['Department of Electronic Engineering', 'Hanyang University, Seoul, Republic of Korea', 'This study presents a method for improving the perfor-', 'mance of the text-to-speech (TTS) model by using three global', 'speech-style representations: language, speaker, and prosody.', ""Synthesizing different languages and prosody in the speaker's"", 'voice regardless of their own language and prosody is possi-', 'ble. To construct the embedding of each representation condi-', 'tioned in the TTS model such that it is independent of the other', 'representations, we propose an adversarial training method for', 'the general architecture of TTS models. Furthermore, we in-', 'troduce a sequential training method that includes rehearsal-', 'based continual learning to train complex and small amounts of', 'data without forgetting previously learned information. The ex-', 'perimental results show that the proposed method can generate', 'good-quality speech and yield high similarity for speakers and', 'prosody, even for representations that the speaker in the dataset', 'does not contain.', 'Index Terms: text-to-speech, cross-lingual, prosody, adversar-', 'ial training, continual learning', '. Introduction', 'Recent end-to-end text-to-speech (TTS) systems [1-2] have', 'achieved good results in terms of generating human-like speech.', 'TTS models have exhibited the ability to transfer style represen-', 'tations such as speakers [3-4], language [5], and prosody [6].', 'By extending such models, many researchers have investigated', 'multispeaker multilingual [7-8] and multispeaker prosody [9]', 'models.', 'Because these representations are prone to entan-', 'glement, these are learned separately by models. In [7], the', ""speaker's identity was preserved during language conversion by"", 'using the L1 consistency loss term.', 'In [10], the adversarial', 'loss was applied to extract language-independent speaker em-', 'bedding and speaker-independent language embedding.', 'In previous works, transfer learning methods have been em-', 'ployed to adapt to new speakers or languages by using a small', 'amount of data [4], [11-12]. One of the methods for speaker or', 'language adaptation is fine-tuning the pre-trained model. This', 'approach can reduce training costs and improve speaker-speech', 'quality by using a limited amount of data.', 'However, when', 'the model is fine-tuned to adapt to new speakers, previously', 'learned information can disappear; this is referred to as the', 'catastrophic forgetting problem. Continual learning approaches', 'have been utilized to solve this problem [13]; these are catego-', 'rized into three types: regularization-based [14], data-rehearsal-', 'based [15-16], and parameter-isolation-based [17]. In [18], con-', 'tinuous speaker adaptation based on a data-rehearsal-based con-', 'tinual learning method was proposed using only one speaker', 'sample per batch. In [19], continual learning of multilingual', 'TTS synthesis was proposed to add new languages by employ-', 'ing random sampling and weighted sampling based on data re-', 'play. These methods can continually add new information with', 'small amounts of data to a well-trained pre-trained TTS model', 'and successfully preserve the previous information.', 'In this paper, we introduce novel methods for synthesiz-', 'ing speech, including three global speech-style representations:', 'language, speaker, and prosody.', 'Synthesizing different lan-', 'guages and prosody while maintaining speaker identity regard-', 'less of their own language and prosody is possible. Our contri-', 'butions are summarized as follows:', '• We design a general architecture for TTS, including lan-', 'guage, speaker, and prosody representations.', '• We propose an efficient method for training each repre-', 'sentation disentangled with other representations using', 'adversarial training.', '• We present a novel sequential training of the rehearsal-', 'based continual learning technique, which trains repre-', 'sentations one-at-a-time even when the amount of data', 'is unbalanced and small, and the representation is com-', 'plex.', '. Methodology', 'Figure 1: Baseline model architecture for synthesizing speech', 'with three global representations.', '.1. Baseline', 'Fig. 1 presents the baseline model architecture for synthesiz-', 'ing speech with three global representations: language, speaker,', 'and prosody. The model comprises two parts: a speech synthe-', 'sizer and representation embeddings. The speech synthesizer is', 'based on Tacotron 2 [2], which is an attention-based sequence-', 'to-sequence model that predicts a mel spectrogram from an in-', 'put text sequence. The representation embeddings conditioned', 'on the speech synthesizer indicate embeddings that express', 'the speech style, including language, speaker, and prosody.', 'The language, speaker, and prosody embeddings represent lan-', 'guage, speaker, and prosody information, respectively. During', 'training, the speaker and language embeddings are obtained as', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-865']"
gu22_interspeech	['National Engineering Laboratory for Speech and Language Information Processing,', 'University of Science and Technology of China, Hefei, China']
huang22e_interspeech	['Department of Computer Science and Information Engineering, National Cheng Kung University,', 'Tainan, Taiwan']
zhang22p_interspeech	['Department of Information and Electronic Engineering, Zhejiang University, China']
zhen22_interspeech	['Alexa AI, Amazon, USA†', 'Hardware Compute Group, Amazon, USA⋆']
li22j_interspeech	['TAL Education Group, Beijing, China']
zhang22q_interspeech	['Department of Information and Electronic Engineering, Zhejiang University, China']
kumar22_interspeech	['Varma, Anurag Dwarakanath and Aram Galstyan', 'Alexa AI, Amazon']
afshan22_interspeech	['Department of Electrical and Computer Engineering, University of California Los Angeles, USA']
afshan22b_interspeech	['Department of Electrical and Computer Engineering, University of California Los Angeles, USA']
shi22c_interspeech	['Toyota Technological Institute at Chicago', 'Meta AI']
tian22b_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China']
ahn22b_interspeech	['School of Computer Science and Engineering, Nanyang Technological University', ', Nanyang', 'Avenue, Singapore', 'School of Electrical and Electronic Engineering, Nanyang Technological University', ', Nanyang', 'Avenue, Singapore', 'National Computer Systems, Singapore']
sang22_interspeech	['Center for Robust Speech Systems, University of Texas at Dallas, TX 75080']
fu22b_interspeech	['Bytedance AI-Lab']
wu22f_interspeech	['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Research Asia', ' Microsoft Azure Speech']
valin22_interspeech	['♮Amazon Web Services', '♭University of Illinois at Urbana-Champaign']
hansen22_interspeech	['Center for Robust Speech Systems (CRSS), University of Texas at Dallas, TX 75080']
hwang22_interspeech	['University of Tsukuba', 'Seikei University', 'Sophia University']
zhu22c_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics CAS, China', 'University of Chinese Academy of Sciences, China', 'Microsoft Research Asia, China']
subramani22_interspeech	['♭University of Illinois at Urbana-Champaign', '♮Amazon Web Services']
cai22_interspeech	['National Engineering Research Center for Multimedia Software, School of Computer Science,', 'Wuhan University, Wuhan, China', 'Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan, China']
fan22_interspeech	['Institute of Automation, Chinese Academy of Sciences, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'Bytedance AI LAB']
li22k_interspeech	['CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,', 'Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems,', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China,', 'Institute of Software, Chinese Academy of Sciences, Beijing, China']
gao22_interspeech	['China Mobile Research Institute']
wu22g_interspeech	['Graduate Institute of Communication Engineering, National Taiwan University', 'Centre for Perceptual and Interactive Intelligence, The Chinese University of Hong Kong', 'Human-Computer Communications Laboratory, The Chinese University of Hong Kong']
kim22h_interspeech	['Qualcomm AI Research†, Qualcomm Korea YH, Seoul, Republic of Korea', 'Seoul National University, Seoul, Republic of Korea']
tian22c_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China', 'Tencent AI Lab', 'Tencent ASR Oteam']
ju22_interspeech	['AIRS Company, Hyundai Motor Group, Seoul, Republic of Korea', 'Carnegie Mellon University, Pittsburgh, PA, USA']
li22l_interspeech	['Information Science and Engineering Institute, Xinjiang University', 'Center for Speech and Language Technologies, Tsinghua University']
thithuuyen22_interspeech	['Posts and Telecommunications Institute of Technology, Hanoi, Vietnam', 'Independent Researcher, Ho Chi Minh City, Vietnam']
deng22b_interspeech	['University of Chinese Academy of Sciences, China', 'Carnegie Mellon University, USA']
lee22h_interspeech	['Department of Electronic Engineering Hanyang University Seoul, Republic of Korea']
ng22_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong', 'Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University']
wang22m_interspeech	['Nankai University', 'Microsoft']
yoon22_interspeech	['Sungkyunkwan University, Seoul, Korea', 'RAONDATA, Seoul, Korea', 'Texas Tech University, Texas, USA', 'University of California, Los Angeles, USA']
zhuang22b_interspeech	['Tencent Music Entertainment Lyra Lab, Shenzhen, China', 'Carnegie Mellon University, Pittsburgh, PA, USA']
yang22j_interspeech	['Qualcomm AI Research†']
yang22k_interspeech	['Tokyo Institute of Techonology', 'Japan Advanced Institute of Science and Technology', 'National Institute of Information and Communications Technology', 'University of Yamanashi']
chen22f_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese', 'Academy of Sciences, China', 'University of Chinese Academy of Sciences, China', 'Tsinghua University, Beijing, China']
wang22n_interspeech	['University of Maryland, College Park, USA']
yang22l_interspeech	['Qualcomm AI Research†, Qualcomm Korea YH, Seoul, Republic of Korea', 'Seoul National University, Seoul, Republic of Korea']
park22c_interspeech	['Department of Electronic Engineering, Gangneung-Wonju National University, Gangneung,', 'South Korea', 'Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD,', 'USA']
guo22d_interspeech	['*The Chinese University of Hong Kong, Hong Kong SAR, China', '‡ Xiaohongshu Inc., Shanghai, China', '†Microsoft Research Asia, Beijing, China']
byun22_interspeech	['Intelligent Signal Processing Lab., Yonsei University, Wonju, Korea', 'Electronics and Telecommunications Research Institute (ETRI), Daejeon, Korea']
mitra22_interspeech	['Apple']
chang22d_interspeech	['Department of Electronic Engineering', 'Hanyang University, Seoul, Republic of Korea']
xue22c_interspeech	['School of Computer Science, Northwestern Polytechnical University, Xian, China', 'Fuxi AI Lab, NetEase Inc., Hangzhou, China']
liu22l_interspeech	['Changhong AI Research (CHAIR), Sichuan Changhong Electric Co., Ltd.']
kim22i_interspeech	['School of Electrical Engineering and Computer Science,', 'Gwangju Institute of Science and Technology, Gwangju 61005, Korea']
li22m_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong']
alicehajic22_interspeech	['University of Amsterdam']
rho22_interspeech	['Sungkyunkwan University', 'Graduate School of AI, POSTECH']
saijo22c_interspeech	['Department of Communications and Computer Engineering, Waseda University, Tokyo, Japan']
guo22e_interspeech	['School of Information, Renmin University of China, P.R.China', 'Language Technologies Institute, Carnegie Mellon University, U.S.A.']
ouyang22_interspeech	['Concordia University, Canada', 'Postal Savings Bank of China, China']
pattanayak22_interspeech	['Department of Electronics and Communication Engineering,', 'National Institute of Technology Patna, India']
ren22_interspeech	['Microsoft Research Asia']
hu22e_interspeech	['Hithink RoyalFlush AI Research Institute, Zhejiang, China']
kim22j_interspeech	['Yonsei University, Department of Electrical and Electronic Engineering, Seoul, South Korea']
park22d_interspeech	['NVIDIA']
huang22g_interspeech	['Graduate Institute of Communication Engineering, National Taiwan University', 'College of Electrical Engineering and Computer Science, National Taiwan University']
wang22p_interspeech	['Kuaishou Technology Co., Beijing, China']
dekorte22_interspeech	['ReadSpeaker']
gao22b_interspeech	['Speech Lab, Alibaba Group, China', 'ICT Cluster, Singapore Institute of Technology, Singapore']
hung22_interspeech	['National Taiwan University', 'Microsoft Corporation', 'Academia Sinica']
fan22b_interspeech	['Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Department of English Language and Literature, Hong Kong Baptist University', 'Institute of Corpus Studies and Applications, Shanghai International Studies University']
dai22_interspeech	['Tencent AI Lab', 'Peking University']
li22n_interspeech	['Google LLC, USA']
conneau22_interspeech	['△Google Research', '♣Hugging Face', 'Meta AI']
liu22n_interspeech	['School of Information and Electronics, Beijing Institute of Technology, Beijing, China', 'Xiaomi Inc., Beijing, China']
dong22b_interspeech	['ByteDance AI Lab', 'Department of Computer Science and Engineering,', 'Southern University of Science and Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China']
jung22b_interspeech	['School of Electrical Engineering, KAIST, Daejeon, Republic of Korea']
fan22c_interspeech	['Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Department of English Language and Literature, Hong Kong Baptist University']
qin22b_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and', 'Computing, Tianjin University, Tianjin, China', 'National Institute of Information and Communications Technology, Kyoto, Japan', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
quan22b_interspeech	['Zhejiang University, Hangzhou, China', 'Westlake University & Westlake Institute for Advanced Study, Hangzhou, China']
chen22g_interspeech	['Harbin Institute of Technology, China', 'Microsoft Corporation']
peng22b_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong', 'Meituan']
tian22d_interspeech	['Bytedance AI-Lab']
hwang22b_interspeech	['Intelligent Signal Processing Lab, Yonsei University, Wonju, Korea.', 'Gangneung-Wonju National University, Gangneung, Korea.']
xu22f_interspeech	['Electronic & Elect. Engineering, Trinity College Dublin, Ireland', 'vivo AI Lab, China', 'School of Foreign Languages, Hubei University of Chinese Medicine, China']
shao22_interspeech	['Westlake University & Westlake Institute for Advanced Study, Hangzhou, China', 'Centre for Speech Technology Research (CSTR), University of Edinburgh, Edinburgh, UK']
shi22d_interspeech	['Carnegie Mellon University', 'Renmin University of China', 'The University of Hong Kong,', 'Human Dataware Lab. Co., Ltd.', 'Nagoya University', 'Tsinghua University,', 'University of California, Berkeley']
xu22g_interspeech	['E.E. Engineering, Trinity College Dublin, Ireland', 'vivo AI Lab, P.R. China', 'School of Foreign Languages, Hubei University of Chinese Medicine, P.R. China']
violeta22_interspeech	['Nagoya University, Japan']
levkovitch22_interspeech	['Tel Aviv University', 'Facebook AI Research']
karakasidis22_interspeech	['Department of Signal Processing and Acoustics, Aalto University, Finland']
leemann22_interspeech	['Center for the Study of Language and Society, University of Bern']
wang22q_interspeech	['School of Information Science and Engineering, Shandong Normal University, China', 'School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China', 'Peking University Huilongguan Clinical Medical School, Beijing Huilongguan Hospital, China', 'Beijing Huilongguan Hospital, Peking University Huilongguan Clinical Medical School, China']
sharma22_interspeech	['Prime Video Compliance and Classification, Amazon.com', 'Prime Video International Expansion, Amazon.com']
zhou22d_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'Tencent AI Lab, Tencent, Shenzhen, China']
wang22r_interspeech	['Institute for Infocomm Research (I2R), A⋆STAR, Singapore']
xin22_interspeech	['ADSPLAB, School of ECE, Peking University, Shenzhen, China']
feng22b_interspeech	['Signal Analysis and Interpretation Lab (SAIL), University of Southern California']
zhou22e_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'Tencent AI Lab, Tencent, Shenzhen, China']
futami22_interspeech	['Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan']
lee22i_interspeech	['Department of Electronic Engineering', 'Hanyang University, Seoul, Republic of Korea', 'KT Corp. Seoul, Republic of Korea']
chaubey22_interspeech	['Data Science, LG Ads Solutions, Mountain View, California, USA']
wei22c_interspeech	['Northwestern Polytechnical University, Xian, China', 'Mashang Consumer Finance Co., Ltd.']
yang22m_interspeech	"[""School of Computer Science, Northwestern Polytechnical University, Xi'an, China""]"
chen22h_interspeech	['Beijing University of Posts and Telecommunications', 'Kuaishou Technology Co., Beijing, China']
saeki22d_interspeech	['Waseda University, Japan', 'Chiba Institute of Technology, Japan']
han22c_interspeech	['National Engineering Research Center for Multimedia Software, School of Computer Science,', 'Wuhan University, Wuhan 430072, China', 'Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan', 'University, Wuhan 430072, China']
kawa22_interspeech	['Wroclaw University of Science and Technology, Wroclaw, Poland']
liu22o_interspeech	['Changhong AI Research (CHAIR), Sichuan Changhong Electric Co., Ltd.']
jiang22_interspeech	['Communication University of China, Beijing, China', 'Microsoft Research Asia, Beijing, China']
li22o_interspeech	['Japan Advanced Institute of Science and Technology, Ishikawa, Japan', 'National Institute of Information and Communications Technology, Kyoto, Japan', 'Tianjin University, Tianjin, China and 4National Institute of Informatics, Tokyo, Japan']
aminidigehsara22_interspeech	['B¨arhold2, Petr Schaffer2, Dirk Plettemeier2, Peter Birkholz1', 'Institute of Acoustics and Speech Communication, Technische Universit¨at Dresden, Germany', 'Institute of Communication Technology, Technische Universit¨at Dresden, Germany']
coppietersdegibson22_interspeech	['Idiap Research Institute, Martigny, Switzerland,', 'Ecole polytechnique federale de Lausanne (EPFL), Switzerland']
feng22c_interspeech	['Shenzhen International Graduate School, Tsinghua University, China', 'Tencent AI lab, ShenZhen', 'Peng Cheng Laboratory']
arai22_interspeech	['Sophia University (Tokyo, Japan)', 'Pannasonic (Japan)']
getman22_interspeech	['Department of Signal Processing and Acoustics, Aalto University, Finland', 'Signal processing, Norwegian University of Science and Technology, Norway', 'Department of Clinical Science, Intervention and Technology, Karolinska Institutet, Sweden']
muckenhirn22_interspeech	['Tagliasacchi1, Scott Wisdom1, John R. Hershey1', 'Google Research', 'Skolkovo Institute of Science and Technology']
he22c_interspeech	['University of Science and Technology of China, HeFei, China', 'Georgia Institute of Technology, Atlanta, GA, USA']
schuppler22_interspeech	['Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria']
sato22_interspeech	['NHK Science and Technology Research Laboratories, Japan', 'NHK Engineering System, Japan', 'Waseda University, Japan']
finkelstein22_interspeech	['Alexey Petelin, Jonathan Shen∗, Vincent Wan, Yu Zhang, Yonghui Wu, Rob Clark', 'Google LLC, †DeepMind']
stephenson22_interspeech	['Universite Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab', ' Grenoble, France', 'LIG, UGA, G-INP, CNRS, INRIA, Grenoble, France', 'NAVER LABS Europe, Meylan, France']
yoshioka22_interspeech	['Nagoya University, Japan', 'AI Inc., Japan']
langheinrich22_interspeech	['Institute of Acoustics and Speech Communication, Technische Universit¨at Dresden, Germany']
ke22_interspeech	['The Hong Kong Polytechnic University, Hong Kong SAR', 'The Chinese University of Hong Kong, Hong Kong SAR']
li22p_interspeech	['Westlake Institute for Advanced Study & 2Westlake University , Hangzhou, China']
nam22_interspeech	['Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Korea']
shin22b_interspeech	['Neosapience, Inc.']
valentinibotinhao22_interspeech	['The Centre for Speech Technology Research, University of Edinburgh, UK', 'SpeakUnique Ltd., UK', 'Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Sweden']
song22d_interspeech	['NAVER Corp., Seongnam, Korea', 'LINE Corp., Tokyo, Japan']
meyer22_interspeech	['Human Language Technology and Pattern Recognition, Computer Science Department,', 'RWTH Aachen University', ' Aachen, Germany', 'AppTek GmbH', ' Aachen, Germany']
fong22_interspeech	['The Centre for Speech Technology Research, University of Edinburgh, UK', 'Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Sweden']
zuo22b_interspeech	['National Key Laboratory for Novel Software Technology,', 'Department of Computer Science and Technology, Nanjing University, China']
liu22p_interspeech	['NERC-SLIP, University of Science and Technology of China, Hefei, China', 'iFLYTEK Research, Hefei, China']
zhang22r_interspeech	['Speech-Language-Hearing Center, School of Foreign Languages, Shanghai Jiao Tong University,', 'Shanghai, China', '* Corresponding author']
tamm22_interspeech	['Laboratory for Cognitive Neurology, KU Leuven, Belgium', 'Processing Speech and Images, KU Leuven, Belgium']
zhang22s_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, China', 'University of Chinese Academy of Sciences, Beijing, China']
chen22i_interspeech	['Research Center for Information Technology Innovation, Academia Sinica, Taiwan']
liu22q_interspeech	['Speech, Hearing and Phonetic Sciences, University College London, U.K.', 'ENT Department, Nottingham University Hospitals, U.K.']
westhausen22_interspeech	['Communication Acoustics & Cluster of Excellence Hearing4all', 'Carl von Ossietzky University, Oldenburg, Germany']
choi22e_interspeech	['Nagoya University, Japan']
udupa22_interspeech	['Electrical Engineering, Indian Institute of Science (IISc), Bangalore-560012, India']
dao22_interspeech	['VinAI Research, Vietnam', 'The University of Melbourne, Australia']
stafylakis22_interspeech	['Omilia - Conversational Intelligence, Athens, Greece']
omahony22_interspeech	['The Centre for Speech Technology Research, University of Edinburgh, UK']
rybicka22_interspeech	['AGH University of Science and Technology, Institute of Electronics, Krakow, Poland', 'Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA', 'Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, USA']
klejch22_interspeech	['Centre for Speech Technology Research, University of Edinburgh, United Kingdom']
tran22b_interspeech	['VinAI Research, Hanoi, Vietnam']
close22_interspeech	['UKRI CDT for Speech and Language Technologies and their Applications,', 'Department of Computer Science, University of Sheffield, Sheffield, United Kingdom']
borsdorf22_interspeech	['Machine Listening Lab (MLL), University of Bremen, Germany', 'Cognitive Systems Lab (CSL), University of Bremen, Germany', 'The Chinese University of Hong Kong, Shenzhen, China']
mirheidari22_interspeech	"['∗Joint First Authors', 'Department of Computer Science, University of Sheffield, UK', ""Institute of Psychiatry, Psychology & Neuroscience (IoPPN), King's College London, London, UK""]"
zhao22j_interspeech	['Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK', 'Tencent AI Lab, Bellevue, WA, USA']
zaiem22_interspeech	['LTCI, Telecom Paris, Institut Polytechnique de Paris', 'LIA, Avignon Universite']
lee22j_interspeech	['Department of Electronic Engineering Hanyang University Seoul, Republic of Korea']
bilinski22_interspeech	['Amazon Alexa']
ridouane22_interspeech	['Laboratoire de Phonetique et Phonologie (CNRS & Sorbonne Nouvelle), Paris']
eom22_interspeech	['School of Electrical Engineering, KAIST, Daejeon, Republic of Korea']
mirheidari22b_interspeech	['Department of Computer Science, University of Sheffield, UK', 'Sheffield Institute for Translational Neuroscience (SITraN); Department of Neuroscience,', 'University of Sheffield, UK']
woszczyk22_interspeech	['Department of Computing, Imperial College London, London, UK', 'Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany']
li22q_interspeech	['School of Software Engineering, Tongji University, P.R.China']
zhang22t_interspeech	"[""Northwestern Polytechnical University, Xi'an, China""]"
zhang22u_interspeech	['Tokyo Institute of Technology, Tokyo, Japan', 'Tencent Cloud Xiaowei, Beijing, China']
wang22s_interspeech	"[""Xi'an Jiaotong University"", 'Microsoft Research Asia, Beijing, China']"
zhao22k_interspeech	['Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory']
mdhaffar22_interspeech	['LIA - Avignon Universite, France']
chen22j_interspeech	['Bytedance AI-Lab']
markitantov22_interspeech	['St. Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences,', 'St. Petersburg Federal Research Center of the Russian Academy of Sciences (SPC RAS)', 'ITMO University, St. Petersburg, Russia']
kim22k_interspeech	['AI Lab, Kakao Enterprise']
du22c_interspeech	['Singapore University of Technology and Design, Singapore', 'National University of Singapore, Singapore', 'The Chinese University of Hong Kong, Shenzhen, China']
shin22c_interspeech	['School of Industrial and Management Engineering, Korea University, Seoul, Republic of Korea']
zhang22v_interspeech	['Department of Computer Science, Inner Mongolia University, China']
chen22k_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Tencent Ethereal Audio Lab, Shenzhen, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China']
yang22n_interspeech	"['Audio, Speech and Language Processing Group, School of Computer Science, Northwestern', ""Polytechnical University, Xi'an, China"", 'Tencent Technology Co., Ltd, Beijing, China']"
zhang22w_interspeech	"[""Northwestern Polytechnical University (NPU), Xi'an, China""]"
yang22o_interspeech	['Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan', 'National Institute of Information and Communications Technology (NICT), Kyoto, Japan', 'Kuaishou Technology, Beijing, China']
wang22t_interspeech	['Department of Computer Science and Technology, Tongji University', 'Department of Computer Science and Engineering, Southern University of Science and Technology', 'School of Data Science, The Chinese University of Hong Kong (Shenzhen)', 'Microsoft 5Peng Cheng Laboratory 6ByteDance AI Lab']
mostaani22_interspeech	['Idiap Research Institute, Martigny, Switzerland', 'Ecole polytechnique federale de Lausanne, Switzerland']
ribeiro22_interspeech	['Universite de Lorraine, CNRS, Inria, LORIA, F-54000, Nancy, France']
dang22_interspeech	['Department of Computer Science and Technology, University of Cambridge, UK']
yang22p_interspeech	"['Harbin Institute of Technology Shenzhen, Shenzhen, China', ""Huawei Noah's Ark Lab""]"
li22r_interspeech	['University of Science and Technology of China, Hefei, P.R.China', 'iFlytek Research, Hefei, P.R.China']
yu22_interspeech	['Research Center for Information Technology Innovation, Academia Sinica, Taiwan', 'Mila-Quebec AI Institute, Montreal, Canada']
suzuki22_interspeech	['Nara Institute of Science and Technology, Japan']
zhang22x_interspeech	"['School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'NLPR, Institute of Automation, Chinese Academy of Sciences, China', 'CAS Center for Excellence in Brain Science and Intelligence Technology, China', ""Huawei Noah's Ark Lab, Shenzhen, China""]"
wei22d_interspeech	['School of Computer Science and Technology, Xidian University']
gorai22_interspeech	['The University of Tokyo, Japan']
mallolragolta22_interspeech	['EIHW – Chair of Embedded Intelligence for Health Care & Wellbeing, University of Augsburg, Germany', 'MTG – Music Technology Group, Universitat Pompeu Fabra, Spain', 'Joint Research Centre, European Commission, Spain', 'Center for Interdisciplinary Health Research, University of Augsburg, Germany', 'GLAM – Group on Language, Audio, & Music, Imperial College London, UK']
lim22_interspeech	['Kakao Enterprise Corporation, Seongnam, Republic of Korea']
magistro22_interspeech	['Gent University']
chen22l_interspeech	['Beijing Language and Culture University, China', 'Smart Platform Product Department,Tencent Technology Co., Ltd, China']
wen22_interspeech	['School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China']
shu22_interspeech	['Speech, Audio and Music Intelligence (SAMI) group, ByteDance']
wang22u_interspeech	"['Soochow University', 'ByteDance AI Lab', 'ABSTRACT', 'Recently, phonetic posteriorgrams (PPGs) based methods have been', 'quite popular in non-parallel singing voice conversion systems.', 'However, due to the lack of acoustic information in PPGs, style', 'and naturalness of the converted singing voices are still limited.', 'To solve these problems, in this paper, we utilize an acoustic ref-', 'erence encoder to implicitly model singing characteristics.', 'We', 'experiment with different auxiliary features, including mel spec-', 'trograms, HuBERT, and the middle hidden feature (PPG-Mid) of', 'pretrained automatic speech recognition (ASR) model, as the input', 'of the reference encoder, and finally find the HuBERT feature is', 'the best choice. In addition, we use contrastive predictive coding', '(CPC) module to further smooth the voices by predicting future', 'observations in latent space. Experiments show that, compared with', 'the baseline models, our proposed model can significantly improve', 'the naturalness of converted singing voices and the similarity with', 'the target singer. Moreover, our proposed model can also make the', 'speakers with just speech data sing.', 'Index Terms— Singing voice conversion, phonetic posterior-', 'grams, acoustic reference, contrastive predictive coding', '. INTRODUCTION', 'Singing voice conversion is intended to change the timbre of singing', 'voices while keeping the musical contents like melody and lyrics', 'maintained. It has achieved more and more attention in recent years,', ""since it can be used in many aspects, like improving the singer's"", 'vocal quality and creating a virtual singer.', 'The early studies are mainly focused on parallel singing', 'voice conversion, which means there are parallel training data', 'between the source singer and the target singer. These traditional', 'methods[1][2][3] use statistical models to build the mapping be-', ""tween the parallel samples. However, it's quite expensive to collect"", 'parallel samples of different singers in real application scenarios. So', 'recently, the studies of non-parallel voice conversion have been the', 'mainstream. The key idea of non-parallel singing voice conversion', 'is to extract the singer-independent content representation first, and', 'then generate the converted singing voices using the target singer', 'embedding. According to whether the singer-independent content', 'representation is extracted in an unsupervised manner, non-parallel', 'singing voice conversion can be categorized into auto-encoder based', 'and PPGs based methods.', 'Auto-encoder based methods disentangle the content feature', 'from speech in an unsupervised manner by imposing constraints', 'on encoder output. Domain confusion [4] module is used in [5][6]', 'to remove speaker-dependent information like timbre from speech.', 'In [7][8], feature disentanglement is realized using variational', 'auto-encoder (VAE). [9] utilizes vector quantized variational auto-', 'encoder (VQ-VAE) as an information bottleneck to get the content', 'information. Although auto-encoder based methods can achieve ac-', 'ceptable results in non-parallel singing voice conversion, its ability', 'of feature disentanglement is not robust enough for various singing', 'input.', 'In PPGs based methods[10][11] [12][13][14], they use the last', 'layer output of the pretrained ASR model, namely PPGs, to repre-', 'sent content information. Since the ASR model is pretrained with', 'huge amount of training data and the training targets are phoneme', 'labels, its ability of eliminating acoustic information is quite robust.', 'However, compared with speech, singing voice contains much more', 'characteristics like prosody and rhythm, which are difficult to model', 'explicitly only using PPGs. Several works have tried to supplement', 'acoustic features in singing voice conversion. In [13], they use pitch', 'to represent melody. However, pitch itself is still not enough to', 'model singing characteristics. In [14], apart from pitch, they feed', 'mel spectrograms into reference encoder to model singing charac-', 'teristics implicitly. However, as mentioned in auto-encoder based', 'methods, there exists too much singer-dependent information in mel', 'spectrograms, which is difficult to remove and will finally degrade', 'the performance.', 'In this work, our contribution is two-fold:', '(1) We try to', 'find an appropriate auxiliary feature that contains enough singer-', 'independent acoustic and musical information to serve as the ref-', 'erence encoder input.', 'Apart from mel spectrograms, We select', 'HuBERT[15], which is a kind of feature originally proposed in', 'self-supervised speech representation learning, and middle hidden', 'feature of ASR model, denoted as PPG-Mid. Both HuBERT feature', 'and PPG-Mid feature contain linguistic and acoustic information.', 'Meanwhile, they contain less singer information than mel spectro-', 'grams. In the experiments, we compare these three features when', 'they serve as the reference encoder inputs.', 'To further eliminate', 'the remaining singer-dependent information like timbre in the ref-', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-10316']"
wei22e_interspeech	['Northwestern Polytechnical University, Xian, China', 'Cloud Xiaowei, Tencent, Beijing, China']
do22_interspeech	['Cambridge Research Laboratory, Toshiba Europe Limited, Cambridge, U.K.']
ludusan22_interspeech	['Phonetics Workgroup, Faculty of Linguistics and Literary Studies & CITEC, Bielefeld University,', 'Germany']
triantafyllopoulos22_interspeech	['Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany', 'GLAM – Group on Language, Audio, & Music, Imperial College, UK', 'Department of Cardiology, Respiratory Medicine and Intensive Care, University Hospital', 'Augsburg, University of Augsburg, Germany', 'Ludwig-Maximilians-University Munich, Germany']
bhat22_interspeech	['TCS Research and Innovation, India', 'Centre for Language and Speech Technology (CLST), Radboud University Nijmegen, The', 'Netherlands', 'Centre for Language Studies (CLS), Radboud University Nijmegen, The Netherlands', 'Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, The', 'Netherlands']
comini22_interspeech	['Amazon Alexa, TTS Research']
kim22l_interspeech	['Department of Electronics Engineering', 'Hanyang University, Seoul, Republic of Korea']
kuang22_interspeech	['Xiaomi Corp., Beijing, China']
li22s_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding,', 'Institute of Acoustics, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China']
suzuki22b_interspeech	['Graduate School of Science and Engineering, Doshisha University.']
tan22_interspeech	['Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong']
gao22c_interspeech	['Dept. of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong', 'Kong SAR, China']
yang22q_interspeech	['National Institute of Advanced Industrial Science and Technology (AIST)']
kothapally22_interspeech	['∗Center for Robust Speech Systems (CRSS), The University of Texas at Dallas, TX, USA', '†Tencent AI Lab, Bellevue, WA, USA']
zhang22y_interspeech	['MoE Key Lab of AI, X-LANCE Lab, CSE Dept, Shanghai Jiao Tong University, Shanghai, China', 'Microsoft Corporation']
yue22b_interspeech	"['University of Science and Technology of China, HeFei, China', ""Huawei Noah's Ark Lab""]"
ao22_interspeech	['School of Data Science, The Chinese University of Hong Kong (Shenzhen)', 'NEL-SLIP, University of Science and Technology of China', 'Microsoft']
defino22_interspeech	['IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France', 'Archean Labs, Montauban, France', 'SILS & GSICCS, Waseda University, Tokyo, Japan']
triantafyllopoulos22b_interspeech	['Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany', 'audEERING GmbH, Gilching, Germany', 'GLAM – Group on Language, Audio, & Music, Imperial College, UK']
dutta22b_interspeech	['Amir H. Poorjam2, Deepak Mittal2, Maneesh Singh2.', 'LEAP Lab, Indian Institute of Science, Bangalore, India.', 'Verisk Analytics, Inc., Jersey City, NJ, USA']
bassan22_interspeech	['Hebrew University of Jerusalem']
wu22h_interspeech	['NVIDIA']
mumtaz22_interspeech	['IIT JAMMU', 'University of Pennsylvania']
lee22k_interspeech	['Department of Linguistics, Seoul National University, Republic of Korea', 'Department of French Language Education, Seoul National University, Republic of Korea']
kulkarni22_interspeech	['Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France.']
bhattacharya22_interspeech	['Gonuguntla3, Murali Alagesan4', 'LEAP lab, Indian Institute of Science, Bangalore, India', 'Ramaiah Medical College Hospital,', 'Bangalore, India', 'General Hospital, Hoskote, Bangalore, India', 'PSG Institute of Medical Sciences', 'and Research, India']
fara22_interspeech	"['Thymia, London , UK', ""Institute of Psychiatry, Psychology & Neuroscience (IoPPN), King's College London, London, UK""]"
liu22r_interspeech	['Beijing University of Posts and Telecommunications, Beijing, China', 'JD.com, Beijing, China', 'Peking University, Beijing, China']
machado22_interspeech	['Dept. Computational Linguistics, University of Zurich, Zurich, Switzerland', 'Dept. Phoniatrics and Speech Pathology, Clinic for Otorhinolaryngology, Head and Neck Surgery,', 'University Hospital Zurich (USZ), Zurich, Switzerland']
tu22_interspeech	['University of Sheffield, Department of Computer Science, Sheffield, UK']
hao22_interspeech	['Junyong Hao, Shunzhou Ye, Cheng Lu, Fei Dong, Jingang Liu, Dong Pi']
zhang22z_interspeech	['Huang3, Yanfen Tang4, Yu Wang4, Fujie Zhang4, Shaoxing Zhang5, Aijun Sun6', 'Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese', 'Academy of Sciences, China', 'University of Chinese Academy of Sciences, China', 'School of Information Science and Engineering, Xinjiang University, China', 'Beijing Ditan Hospital Capital Medical University, China', 'Peking University Third Hospital, China', 'Dalian Public Health Clinical Center, China']
chen22m_interspeech	['Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese', 'Academy of Sciences, China', 'University of Chinese Academy of Sciences, China']
yang22r_interspeech	['Cooperative Medianet Innovation Center, Shanghai Jiao Tong University', 'Shanghai AI Laboratory']
kocour22_interspeech	['Jan “Honza” ˇCernocky⋆', '† NTT Corporation, Japan']
tu22b_interspeech	['University of Sheffield, Department of Computer Science, Sheffield, UK']
svec22_interspeech	['Department of Cybernetics, University of West Bohemia, Pilsen, Czech Republic']
jiang22b_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute', 'X-LANCE Lab, Department of Computer Science and Engineering', 'Shanghai Jiao Tong University, Shanghai, China']
bergsma22_interspeech	['⋆Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland', '† Logitech Europe S.A., Lausanne, Switzerland', '⋄Reexen Technology, Zurich, Switzerland']
tao22_interspeech	['Department of Electronic Engineering', 'Department of Educational Psychology', 'The Chinese University of Hong Kong']
wang22v_interspeech	['University of Science and Technology of China, HeFei, China', 'School of Electrical and Computer Engineering, Georgia Institute of Technology, GA, USA']
huang22h_interspeech	['Department of Electrical Engineering, National Tsing Hua University, Taiwan', 'Institute of Communication Engineering, National Tsing Hua University, Taiwan']
javanmardi22_interspeech	['Department of Signal Processing and Acoustics, Aalto University, Finland']
ma22_interspeech	['School of Information Science and Engineering, Xinjiang University, Urumqi, China', 'Tencent Minority-Mandarin Translation, Beijing, China', 'Xinjiang Key Laboratory of Multi-lingual Information Technology, Urumqi, China']
nabe22_interspeech	['Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS, LPNC', ' Grenoble, France', 'Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab', ' Grenoble, France']
liu22s_interspeech	['Alibaba Group, China']
vitormenezes22_interspeech	['Institute of Acoustics and Speech Communication, Technische Universit¨at Dresden', 'Institute of Communication Technology, Technische Universit¨at Dresden']
hollands22_interspeech	['Department of Computer Science, University of Sheffield, UK', 'Sheffield Institute for Translational Neuroscience, University of Sheffield, UK']
chen22n_interspeech	['Department of Electrical Engineering, National Tsing Hua University, Taiwan']
lehecka22_interspeech	['Department of Cybernetics, University of West Bohemia Pilsen, Czech Republic']
shao22b_interspeech	"[""School of Computer Science, Northwestern Polytechnical University, Xi'an, China"", 'Tencent Research, Beijing, China']"
kakoulidis22_interspeech	['⋆Innoetics, Samsung Electronics, Greece', '† Mobile Communications Business, Samsung Electronics, Republic of Korea']
r22_interspeech	['Vinod K Kurmi1', 'Vinay P Namboodiri2', 'CV Jawahar1', 'IIIT Hyderabad, India.', 'University of Bath, UK.']
landini22_interspeech	['AUDIAS (Audio, Data Intelligence and Speech), Universidad Autonoma de Madrid, Spain']
su22_interspeech	['Department of Electrical Engineering, National Tsing Hua University, Taiwan']
kukk22_interspeech	['Laboratory of Language Technology', 'Tallinn University of Technology, Estonia']
chien22_interspeech	['Dept of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan']
robach22_interspeech	['Communication Acoustics, Carl von Ossietzky University, Oldenburg, Germany', 'Fraunhofer IDMT, Hearing, Speech and Audio Technology, Oldenburg, Germany', 'Medizinische Physik, Carl von Ossietzky University, Oldenburg, Germany', 'Cluster of Excellence Hearing4all, Germany']
sapru22_interspeech	['Amazon Alexa, Bangalore, India']
toya22_interspeech	['Japan Advanced Institute of Science and Technology', 'Faculty of Human Sciences, Waseda University', 'Westunitis Co., Ltd.']
xie22_interspeech	['Alexa Machine Learning, Amazon, USA']
liu22t_interspeech	['MoE Key Lab of Artificial Intelligence, AI Institute, X-LANCE Lab, Shanghai Jiao Tong University', 'AISpeech Ltd, Suzhou China']
sonowal22_interspeech	['Samsung Electronics', 'FasterAI']
weise22b_interspeech	['Department of Computer Science, The Graduate Center, CUNY, USA', 'Department of Computer and Information Science, Brooklyn College, CUNY, USA']
um22_interspeech	['School of Electrical Engineering, KAIST, Daejeon, Republic of Korea']
lee22l_interspeech	['Institute of Information Science, Academia Sinica']
cheng22c_interspeech	['School of Information Sciences, Beijing Language and Culture University, Bejing, China', 'Smart Platform Product Department, Tencent Technology Co., Ltd, Shenzhen, China']
chao22_interspeech	['CSIE, NCKU, Taiwan 2CITI, Academia Sinica, Taiwan 3Microsoft Corporation 4NICT, Japan']
kons22_interspeech	['Thomas, George Saon', 'IBM Research AI']
chen22o_interspeech	['Watanabe3, Odette Scharenborg6, Jingdong Chen7, Bao-Cai Yin5, Jia Pan5', 'University of Science and Technology of China, China 2 Georgia Institute of Technology, USA', 'Carnegie Mellon University, USA 4 Kore University of Enna, Italy 5 iFlytek, China', 'Delft University of Technology, The Netherlands7 Northwestern Polytechnical University, China']
flechl22_interspeech	['Nuance Communications Inc., Burlington, USA']
prabhu22_interspeech	['⋆Signal Processing, Universit¨at Hamburg, Germany', '†Industrial and Organizational Psychology, Universit¨at Hamburg, Germany']
pandey22_interspeech	['Department of Computer Science and Engineering, The Ohio State University, USA']
yin22b_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'ICT Products & Solutions, Huawei, Dongguan, China', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
zeng22_interspeech	['National Institute of Informatics, Japan 2SOKENDAI, Japan 3Tianjin University, China']
elbanna22_interspeech	['Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland', 'Logitech Europe S.A., Lausanne, Switzerland', 'Imperial College London, London, United Kingdom', 'Universite de Lausanne, Lausanne, Switzerland']
tan22b_interspeech	['Guangzhou University, Guangzhou, China', 'Peng Cheng Laboratory, Shenzhen, China', 'Zhejiang University of Science and Technology, Hangzhou, China']
xiao22_interspeech	['School of Computer Science and Engineering, Nanyang Technological University, Singapore.']
ou22_interspeech	['College of Intelligence and Computing, Tianjin University, China']
du22d_interspeech	['Graduate School of Informatics, Kyoto University, Japan', 'AIP, RIKEN, Japan', 'National Institute of Advanced Industrial Science and Technology (AIST), Japan', 'Telecom Paris, Institut Polytechnique de Paris, France']
chen22p_interspeech	['School of Computer Science and Engineering, Nanyang Technological University, Singapore']
kadiri22_interspeech	['Department of Signal Processing and Acoustics, Aalto University, Finland']
huckvale22_interspeech	['Speech, Hearing and Phonetic Sciences, University College London, UK']
geng22_interspeech	['Tianjin University', 'School of Electrical Information Engineering', 'Tianjin, China']
fu22c_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Department of Electrical and Computer Engineering, National University of Singapore, Singapore', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
guzik22_interspeech	['Signal Processing Group, Institute of Electronics', 'AGH University of Science and Technology, Krakow, Poland']
li22t_interspeech	['Shanghai International Studies University, Institute of Linguistics']
sarkar22_interspeech	['Idiap Research Institute, Martigny, Switzerland', 'Ecole polytechnique federale de Lausanne, Switzerland']
furukawa22_interspeech	['Nara Institute of Science and Technology, Japan', 'Graduate School of Arts and Sciences, The University of Tokyo, Japan']
eranovic22_interspeech	['McMaster University, Canada', 'University of Novi Sad, Serbia']
tao22b_interspeech	['Department of Electronic Engineering', 'Department of Educational Psychology', 'The Chinese University of Hong Kong']
mahadeokar22_interspeech	['Michael L Seltzer', 'Meta AI, USA']
delvaux22_interspeech	['UMONS, Language Research Institute & FNRS, Belgium', 'University of Lille, SCALab, UMR CNRS 9193, France', 'UMONS, Department of Legal Psychology, Mons, Belgium', 'Research Center in Social Defense, Tournai, Belgium', 'Psychiatric Hospital, Saint-Amand-les-Eaux, France']
kanagawa22_interspeech	['NTT Corporation, Japan']
cui22c_interspeech	['Xiaomi, Beijing, China']
audhkhasi22_interspeech	['Google LLC, New York']
zhang22aa_interspeech	['University of Chinese Academy of Social Sciences (UCASS)', 'Corpus and Computational Linguistics Center, Institute of Linguistics, CASS']
mai22_interspeech	['ML-Labs, School of Computer Science, University College Dublin, Ireland']
liu22u_interspeech	['Maastricht University, The Netherlands 2Meta AI, USA 3Johns Hopkins University, USA']
zheng22c_interspeech	['Speech Processing and Machine Intelligence (SPMI) Lab, Tsinghua University, China', 'Meituan, China']
roy22_interspeech	['Electrical Engineering, Indian Institute of Science (IISc), Bangalore-560012, India']
ide22_interspeech	['Waseda University', 'Intelligent Framework Lab Inc.']
parry22_interspeech	['Speech Graphics Ltd, Edinburgh, United Kingdom']
rathod22_interspeech	['Samsung Research', 'Massachusetts Institute of Technology']
vyas22_interspeech	['Idiap Research Institute, Switzerland', 'Ecole Polytechnique Federale de Lausanne, Switzerland', 'Meta AI']
zhou22f_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Huya Inc., Guangzhou, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'XVerse Inc., Shenzhen, China']
yang22s_interspeech	['Human Language Technology and Pattern Recognition Group, Computer Science Department', 'RWTH Aachen University', ' Aachen, Germany 2AppTek GmbH', ' Aachen, Germany', 'AppTek, McLean, Virginia, USA']
li22u_interspeech	['MIIT Key Lab for Language Information Processing and Applications', 'School of Foreign Studies, Nanjing University of Science and Technology']
li22v_interspeech	['The Chinese University of Hong Kong, Hong Kong, China', 'Lightspeed & Quantum Studios, Tencent, Shenzhen, China']
kim22m_interspeech	['Department of Electronic Engineering, Inha University']
li22w_interspeech	['Peking University', 'Meituan']
yi22b_interspeech	['Mittag3,Ross Cutler3, Zhuohuang Zhang4, Donald S. Williamson4, Fei Chen5, Fuzheng Yang6,', 'Shidong Shang1', 'Tencent Ethereal Audio Lab, China,', 'Technical University of Berlin, Germany,', 'Microsoft Corp., USA,', 'Indiana University Bloomington, USA,', 'Southern University of Science and Technology, China,', 'XiDian University, China', '. Abstract', 'With the advances in speech communication systems such as', 'online conferencing applications, we can seamlessly work with', 'people regardless of where they are. However, during online', 'meetings, speech quality can be significantly affected by back-', 'ground noise, reverberation, packet loss, and network jitter, to', 'name a few. Because of its nature, speech quality is tradition-', 'ally assessed in subjective tests in laboratories and lately also', 'through crowdsourcing following the international standards', 'from the ITU-T Rec. P.800 series. However, those approaches', 'are costly and cannot be applied to customer data. Therefore,', 'an effective objective assessment approach is needed to evaluate', 'or monitor the speech quality of the ongoing conversation. The', 'ConferencingSpeech 2022 challenge targets the non-intrusive', 'deep neural network models for the speech quality assessment', 'task. We open-sourced a training corpus with more than 86K', 'speech clips in different languages, with a wide range of synthe-', 'sized and live degradations and their corresponding subjective', 'quality scores through crowdsourcing. 18 teams submitted their', 'models for evaluation in this challenge. The blind test sets in-', 'cluded about 4300 clips from wide ranges of degradations. This', 'paper describes the challenge, the datasets, and the evaluation', 'methods and reports the final results.', 'Index Terms:', 'speech quality, deep learning, non-intrusive', 'model', '. Introduction', 'With the popularity of remote conferencing, voice-based', 'human-computer interaction has become mainstream.', 'Envi-', 'ronmental noise, room reverberation, digital signal processing,', 'and network transmission can all degrade the quality of the', 'speech signal. In these applications, speech quality assessment', 'is in high demand. So far, the above fields have made great', 'progress. In ITU-T Rec. P.800 [1], the international telecom-', 'munication union develops subjective evaluation procedures to', 'assess speech quality, which is also the most preferred approach', 'for quality assessment. However, it must be performed under', 'controlled conditions, which is often time-consuming and ex-', 'pensive. Meanwhile, the perceptual evaluation of speech qual-', 'ity (PESQ) [2] and perceptual objective listening quality anal-', 'ysis (POLQA) [3] are designed to objectively evaluate speech', 'quality. However, they need clean reference speech signals as', 'comparison input. In order to non-intrusively assess the speech', 'quality, ITU-T Rec. P.563 [4] was developed but only for target', 'narrow-band applications. As deep learning shines in various', 'fields, deep neural networks have been developed to address', 'the non-intrusive speech quality assessment problem recently', '[5', '', '', '', '', '', '', ']. Nevertheless, most of these methods', 'adopt PESQ or POLQA as the speech quality label, which can', 'not really represent the subjective ratings in all impairments.', 'Only a few datasets with subjective scores have been published,', 'which limits the application of deep learning in the above prob-', 'lem. Therefore, a large dataset with subjective speech quality', 'scores and a non-intrusive speech quality assessment method,', 'which can better reflect perceived subjective feelings, are ur-', 'gently needed.', 'The ConferencingSpeech 2022 challenge aims to stimulate', 'research in the above-mentioned areas. We provided compre-', 'hensive training and test datasets that contain at least 200 hours', 'of speech samples with subjective test scores. We hope this', 'challenge helps facilitate idea exchanges and discussions in this', 'special session. Meanwhile, this challenge has the following', 'features: 1) We aim for non-intrusive models for evaluating the', 'speech quality (i.e., without reference speech signals), which', 'is more practical in online conferencing applications. 2) With', 'the continuous expansion of bandwidth in voice communica-', 'tion systems, the existing standardized non-intrusive objective', 'speech quality assessment method for narrowband speech such', 'as defined in ITU-T P.563 is no longer applicable. Therefore,', 'this challenge aims to effectively evaluate the speech quality for', 'signals with broader bandwidth. 3) To truly reflect subjective', 'opinion on speech quality, the training and test datasets con-', 'tain the mean opinion score (MOS), which is obtained through', 'subjective absolute category rating tests via crowdsourcing and', 'in accordance with the ITU-T Rec P.808 [13] using its open-', 'sourced implementation [14]. 4) Different from the Clarity Pre-', 'diction Challenge [15] which evaluates the speech intelligibility', 'of speech signals, this is the first challenge on non-intrusive ob-', 'jective speech quality assessment in an online conferencing. We', 'provide speech clips with subjective MOS that covers most of', 'the impairment scenarios in on-line speech communication. It is', 'believed that this will promote the development of non-intrusive', 'objective speech quality assessment methods.', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-10597']
hu22f_interspeech	['Lyra Lab, Tencent Music Entertainment, Shenzhen, China', 'Centre for Speech Technology Research, The University of Edinburgh, United Kingdom']
rakotomalala22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab', ' Grenoble, France']
pandey22b_interspeech	['Sigmedia Lab, ADAPT Centre, School of Engineering, Trinity College Dublin, Ireland', 'ADAPT Centre, School of Computer Science, University College Dublin, Ireland']
rana22_interspeech	['McMaster University, Hamilton, ON, Canada']
jose22_interspeech	['Amazon Science, United States']
chang22e_interspeech	['Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'Amazon AI, USA']
biadsy22_interspeech	['Google LLC']
zaidi22_interspeech	['Samsung Research, Seoul, South Korea']
schraner22_interspeech	['University of Applied Sciences and Arts Northwestern Switzerland']
lam22_interspeech	['Singapore University of Technology and Design', 'Institute for Infocomm Research, A*STAR, Singapore']
chang22f_interspeech	['National Taiwan University']
sharon22_interspeech	['ShareChat, India']
abderrazek22_interspeech	['LIA, Avignon University, France', 'Aix-Marseille Univ, LPL, CNRS, Aix-en-Provence, France', 'UT2J, LNPL, Toulouse University & Toulouse Hospital, France']
bartolewska22_interspeech	['AGH University of Science and Technology, Institute of Electronics', '-059 Krakow, Poland']
lemaguer22_interspeech	['Sigmedia Lab, ADAPT Centre, School of Engineering, Trinity College Dublin, Ireland', 'The Centre for Speech Technology Research, University of Edinburgh, UK']
yang22t_interspeech	['Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany', 'Educational Physiology Laboratory, University of Tokyo, Japan', 'Device Software Lab, Munich Research Center, Huawei Technologies, Germany', 'GLAM – Group on Language, Audio, & Music, Imperial College London, UK']
patel22_interspeech	['Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands']
luu22_interspeech	['Centre for Speech Technology Research, University of Edinburgh, UK']
gao22d_interspeech	['University of Cambridge', 'Samsung AI', 'Avignon University']
zhou22g_interspeech	['Siniscalchi2,3, Shinji Watanabe4, Odette Scharenborg5, Jingdong Chen6, Shifu Xiong7, Jian-Qing', 'Gao7', 'University of Science and Technology of China, China 2Georgia Institute of Technology, USA', 'Kore University of Enna, Italy 4Carnegie Mellon University, USA 7 iFlytek, China', 'Delft University of Technology, The Netherlands 6Northwestern Polytechnical University, China']
peng22c_interspeech	['Department of Computer Science, The University of Texas at Austin', 'HuBERT', 'Ours', 'Figure 1: HuBERT: sum of attention weights each frame receives from other frames. Ours (VG-HuBERT3): attention weights each', 'frame receives from the [CLS A] token. Attention weights from different attention heads are coded with different colors.']
welker22_interspeech	['Signal Processing (SP), Universit¨at Hamburg, Germany', 'Center for Free-Electron Laser Science, DESY, Hamburg, Germany', '†Authors contributed equally to this work.']
li22x_interspeech	['Peking University', 'University of Oxford', 'Microsoft STCA']
weiran22_interspeech	['Google, Inc.']
oh22_interspeech	['Department of Linguistics, University of Southern California, USA', 'Department of Linguistics, University of Michigan, USA']
quamer22_interspeech	['Department of Computer Science and Engineering, Texas A&M University', 'Department of English, Iowa State University']
wang22w_interspeech	['University of St. Gallen, Switzerland', 'Reykjavik University, Iceland']
bradshaw22_interspeech	['Department of Computational Linguistics, University of Zurich, Switzerland', 'Department of Language and Linguistic Science, University of York, UK']
shirian22_interspeech	['The University of Warwick, UK', 'Google Research, USA', 'The University of Glasgow, UK']
teixeira22_interspeech	['INESC-ID/Instituto Superior Tecnico, University of Lisbon, Portugal', 'LTI, Carnegie Mellon University, USA']
gibson22_interspeech	['Universidad de Navarra', 'Carl von Ossietzky Universitat Oldenburg', 'Universitat de Girona']
hernandez22_interspeech	['Pattern Recognition Lab. Friedrich-Alexander Universit¨at Erlangen-Nurnberg, Germany', 'GITA Lab. Facultad de Ingenieria. Universidad de Antioquia UdeA, Medellin, Colombia', 'Speech & Language Processing Lab. Friedrich-Alexander Universit¨at Erlangen-Nurnberg, Germany']
ardaillon22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, F-38000 Grenoble, France']
chatzoudis22_interspeech	['Athanasios Katsamanis1,2, Vassilis Katsouros1', 'Institute for Language and Speech Processing, Athena Research Center, Athens, Greece', 'Behavioral Signal Technologies, Los Angeles, CA, USA', 'National and Kapodistrian University of Athens, Athens, Greece']
li22y_interspeech	['Cambridge Research Laboratory, Toshiba Europe Ltd, Cambridge, UK']
dejong22_interspeech	['Istituto Italiano di Tecnologia, Center for Translational Neurophysiology of Speech and', 'Communication, (CTNSC), Ferrara, Italy', 'Universita di Ferrara, Dipartimento di Neuroscienze e Riabilitazione, Ferrara, Italy', 'Aix Marseille Univ, CNRS, LPL, Aix-en-Provence, France', 'Institute for Language, Communication and the Brain (ILCB), Marseille, France']
girish22_interspeech	['Observe.AI']
li22z_interspeech	['University of Cambridge', '†These authors have contributed equally to this work and share first authorship']
elhajal22_interspeech	['Logitech Europe S.A., Lausanne, Switzerland', 'Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland']
dinarelli22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG', ' Grenoble, France']
meyer22b_interspeech	['Institute for Natural Language Processing (IMS), University of Stuttgart, Germany']
wang22x_interspeech	['Microsoft Cloud+AI, USA']
ali22_interspeech	['Information Engineering and Computer Science School, University of Trento', ' Trento, Italy', 'Fondazione Bruno Kessler', ' Trento, Italy']
li22aa_interspeech	['Carnegie Mellon University']
singh22_interspeech	['Centre for Vision, Speech and Signal Processing (CVSSP)', 'University of Surrey, UK']
weiran22b_interspeech	['Tara N. Sainath', 'Google, Inc.']
spinu22_interspeech	['City University of New York, Kingsborough Community College', 'Interdisciplinary Laboratory of Digital Sciences, Paris-Saclay University', 'Nemours Biomedical Research']
tzinis22_interspeech	['Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA', 'University of Illinois at Urbana-Champaign, Urbana, IL, USA']
manghat22_interspeech	['IEEE Graduate Member', 'Cognitive Systems Lab, University Bremen, Germany']
zeineldeen22_interspeech	['Human Language Technology and Pattern Recognition, Computer Science Department,', 'RWTH Aachen University', ' Aachen, Germany', 'AppTek GmbH', ' Aachen, Germany']
hutin22_interspeech	['Universite Paris-Saclay, CNRS, LISN', ', Orsay, France', 'Universite Paris 3 Sorbonne-Nouvelle, CNRS, UMR 7018, LPP', ', Paris, France']
kuang22b_interspeech	['Department of Linguistics, University of Pennsylvania, USA', 'Speech-Language-Hearing Center, Shanghai Jiao Tong University, China']
lu22c_interspeech	['Academia Sinica, Taipei', 'Carnegie Mellon University, USA', 'Shanghai Jiao Tong University, Shanghai', 'Universita Politecnica delle Marche, Italy', 'Meta AI, USA', 'LINE Corporation, Japan', 'Tokyo Metropolitan University, Japan']
nguyen22d_interspeech	['Karlsruhe Institute of Technology', 'Carnegie Mellon University']
turetzky22_interspeech	['School of Computer Science and Engineering', 'The Hebrew University of Jerusalem']
sklyar22_interspeech	['Amazon Alexa']
yadavalli22_interspeech	['Speech Processing Laboratory', 'International Institute of Information Technology, Hyderabad', 'Gachibowli, Hyderabad, Telangana']
kuhlmann22_interspeech	['Paderborn University, Germany', 'Bielefeld University, Germany']
vanderreydt22_interspeech	['IDLab Ugent-imec']
ghosh22b_interspeech	['Speech Lab, Department of Electrical Engineering, IIT Madras, Chennai, India', 'MIDAS Labs, IIIT-Delhi, India', 'TEG Analytics, Bangalore, India', 'Cisco Systems, Bangalore, India']
mariotte22_interspeech	['LAUM, UMR CNRS 6613, IA-GS, Le Mans Universite, Av. Olivier Messiaen 72085 Le Mans, France', 'LIUM, Le Mans Universite, Av. Rene Laennec 72085 Le Mans, France']
liebig22_interspeech	['Institute of Acoustics and Speech Communication, Technische Universit¨at Dresden, Germany', 'Department of Audiology and Phoniatrics, Charite-Universit¨atsmedizin Berlin, Germany']
antony22_interspeech	['Department of Computer Science and Engineering, National Institute of Technology, Karnataka,', 'Surathkal, India']
nikitaras22_interspeech	['Innoetics, Samsung Electronics, Greece', 'Mobile Communications Business, Samsung Electronics, Republic of Korea']
becerra22_interspeech	['School of Computer Science, University of College Dublin, Dublin, Ireland']
sato22b_interspeech	['NTT Corporation, Japan']
gudmalwar22_interspeech	['National Institute of Technology Meghalaya, India 793003']
talkar22_interspeech	['Human Health and Performance Systems, MIT Lincoln Laboratory, Lexington, MA, USA', 'Speech and Hearing Bioscience and Technology, Harvard University, Boston, MA, USA', 'Department of Neurology and NeuroNexus Institute, University of Massachusetts Chan Medical', 'School, Worchester, MA, USA']
liao22_interspeech	['gemori1, Felicitas Kleber1, Dirk Voit2, Jens Frahm2, Jonathan Harrington1', 'Institute of Phonetics and Speech Processing, University of Munich', 'Max Planck Institute for Multidisciplinary Sciences']
baumann22_interspeech	['Technische Hochschule Nurnberg Georg Simon Ohm, Germany', 'Intel Labs']
tzudir22_interspeech	['Indian Institute of Technology Guwahati, Guwahati - 781039, India', 'Indian Institute of Technology Dharwad, Dharwad - 580011, India']
fras22_interspeech	['AGH University of Science and Technology, Institute of Electronics, Krakow, Poland']
deoliveira22_interspeech	['Signal Processing (SP), Universit¨at Hamburg, Germany']
benway22_interspeech	['Communication Sciences and Disorders, Syracuse University, New York, USA', 'Haskins Laboratories, New Haven, Connecticut, USA', 'Communication Sciences and Disorders, Montclair State University, New Jersey, USA', 'Electrical Engineering and Computer Science, Syracuse University, New York, USA', 'Communicative Sciences and Disorders, New York University, New York, USA']
lin22d_interspeech	['Emory University', 'Google LLC']
ding22b_interspeech	['Google LLC, USA']
kumar22b_interspeech	['Observe.AI, India']
baby22_interspeech	['Amazon Alexa']
berrebbi22_interspeech	['Language Technologies Institute, Carnegie Mellon University', 'Universidad Nacional Autonoma de Mexico, Iztacala', 'Dept. of Anthropology, Gettysburg College']
siuzdak22_interspeech	['Charactr, Inc.', 'Max-Planck-Institute for Empirical Aesthetics, Frankfurt, Germany']
ravi22_interspeech	['†Dept . of Electrical and Computer Engineering, University of California, Los Angeles, USA', '§Dept. of Psychiatry and Biobehavioral Sciences, University of California, Los Angeles, USA']
le22_interspeech	['Meta, USA']
wang22y_interspeech	['Department of Electrical Engineering-ESAT, KU Leuven, Belgium']
ding22c_interspeech	['Google LLC, USA']
bai22b_interspeech	['Centre for Language and Speech Technology (CLST), Radboud University, The Netherlands', 'Centre for Language Studies (CLS), Radboud University, The Netherlands', 'Donders Institute for Brain, Cognition and Behaviour, Radboud University, The Netherlands']
buech22_interspeech	['Laboratoire de Phonetique et Phonologie, UMR 7018, CNRS/Sorbonne Nouvelle, France', 'IfL Phonetics, University of Cologne, Germany']
wang22z_interspeech	['Dept . of Electrical and Computer Engineering, University of California, Los Angeles, USA', 'Dept. of Psychiatry and Biobehavioral Sciences, University of California, Los Angeles, USA']
zhang22ba_interspeech	['Department of Head and Neck Surgery, University of California, Los Angles']
dheram22_interspeech	['Amazon Alexa AI, USA']
tran22c_interspeech	['Arizona State University, USA', 'Aural Analytics, USA', 'Mayo Clinic, USA']
huang22j_interspeech	['Google Research']
barker22_interspeech	['Graetzer3, Holly Griffiths2, Lara Harris3, Rhoddy Viveros-Munoz4, Graham Naylor2, Zuzanna', 'Podwinska3, Eszter Porter2', 'Department of Computer Science, University of Sheffield, UK', 'School of Medicine, University of Nottingham, UK', 'Acoustics Research Centre, University of Salford, UK', 'School of Psychology, Cardiff University, UK']
lebourdais22_interspeech	['LIUM, Le Mans Universite, France']
diener22_interspeech	['Microsoft Corporation']
buech22b_interspeech	['Laboratoire de Phonetique et Phonologie, UMR 7018, CNRS/Sorbonne Nouvelle, France']
helwani22_interspeech	['Amazon Web Services, Inc., Palo Alto, CA, USA']
yadav22_interspeech	['University of Glasgow, UK', 'Google Research, Paris, France']
damania22_interspeech	['Rochester Institute of Technology, Rochester, NY USA', 'Boston College, Chestnut Hill, MA USA']
chang22g_interspeech	['Carnegie Mellon University, PA, USA', 'Yahoo Japan Corporation, Tokyo, JAPAN']
koch22_interspeech	['Institute for Natural Language Processing (IMS), University of Stuttgart, Germany', 'Institute of Literary Studies (ILS), University of Stuttgart, Germany', 'German Literature Archive (DLA), Marbach, Germany']
deadman22_interspeech	['Department of Computer Science, University of Sheffield, UK']
paturi22_interspeech	['Amazon AWS AI']
radfar22_interspeech	['Alexa Machine Learning, Amazon, USA']
lee22m_interspeech	['Department of Head and Neck Surgery, David Geffen School of Medicine at University of', 'California, Los Angeles, Los Angeles, California, USA', 'Department of Linguistics, University of Michigan, Ann Arbor, Michigan, USA']
chetupalli22_interspeech	['International Audio Laboratories Erlangen∗, Am Wolfsmantel 33', ' Erlangen, Germany']
meyer22c_interspeech	['Chris Emezue11, Jonathan Mukiibi 12, Salomey Osei, Apelete Agbolo13, Victor Akinode,', 'Bernard Opoku14, Samuel Olanrewaju, Jesujoba Alabi2, Shamsuddeen Muhammad', '∀Masakhane NLP, Africa 1 Coqui, USA 2 Saarland University, Germany 3 University of Sao Paulo,', 'Brazil 4 CLEAR Global, USA 5 Col·lectivaT, Spain 6 SIL International 7 Leibniz Universitat', 'Hannover, Germany 8 Johns Hopkins University, USA 9 Niger-Volta LTI, USA', ' Carnegie Mellon', 'University, USA 11 Technical University of Munich, Germany 12 Makerere University, Uganda', 'Ewegbe Akademi, Togo', ' Kwame Nkrumah University of Science and Technology, Ghana']
kapelonis22_interspeech	['School of ECE, National Technical University of Athens, Athens, Greece', 'Institute for Language and Speech Processing, Athena Research Center, Athens, Greece']
laptev22_interspeech	['NVIDIA, USA', 'ITMO University, St. Petersburg, Russia']
vanrijn22_interspeech	['Harrison4, Elisabeth Andre2, Nori Jacoby1', 'Max-Planck-Institute for Empirical Aesthetics, Frankfurt, Germany', 'Human-Centered Artificial Intelligence, Augsburg, Germany', 'Charactr Inc., Los Angeles, US', 'University of Cambridge, Cambridge, UK']
klapsas22_interspeech	['Kakoulidis1, Konstantinos Markopoulos1, Spyros Raptis1, June Sig Sung2, Gunu Jho2, Aimilios', 'Chalamandaris1, Pirros Tsiakoulis1', 'Innoetics, Samsung Electronics, Greece', 'Mobile Communications Business, Samsung Electronics, Republic of Korea']
liu22v_interspeech	['Bose Corporation, USA', 'Department of Computer Science, Indiana University, USA']
sartzetaki22_interspeech	['School of ECE, National Technical University of Athens, Greece', 'Institute for Speech and Language Processing, Athens, Greece']
zhu22d_interspeech	['University of Massachusetts Boston', 'University of North Carolina at Chapel Hill', 'Dartmouth College']
antonova22_interspeech	['NVIDIA']
dieck22_interspeech	['Elmar Noth1, Philipp Klumpp1', 'Pattern Recognition Lab, Friedrich-Alexander-Universitat, Erlangen-Nurnberg, Germany', 'GITA Lab, Facultad de Ingenieria. Universidad de Antioquia, Medellin, Colombia', 'Department of Otorhinolaryngology, Head & Neck Surgery, University Hospital Erlangen']
rose22_interspeech	"['Google Inc., New York', 'ABSTRACT', 'This paper presents a new approach for end-to-end audio-visual', 'multi-talker speech recognition. The approach, referred to here as', 'the visual context attention model (VCAM), is important because', 'it uses the available video information to assign decoded text to', 'one of multiple visible faces.', 'This essentially resolves the label', 'ambiguity issue associated with most multi-talker modeling ap-', 'proaches which can decode multiple label strings but cannot assign', 'the label strings to the correct speakers. This is implemented as a', 'transformer-transducer based end-to-end model and evaluated using', 'a two speaker audio-visual overlapping speech dataset created from', 'YouTube videos. It is shown in the paper that the VCAM model im-', 'proves performance with respect to previously reported audio-only', 'and audio-visual multi-talker ASR systems.', 'Index Terms— audio-visual speech recognition, multi-talker', 'speech recognition', '. INTRODUCTION', 'This paper presents the multi-talker (M-T) visual context attention', 'model (VCAM), an end-to-end (E2E) audio-visual (A/V) modeling', 'approach for transcribing utterances in scenarios where there is over-', 'lapping speech from multiple talkers. The presence of overlapping', 'speech in utterances arising from human-human interaction has been', 'studied in several domains including meetings [1] and call center', 'tasks [2]. In a study of interactions in an example call center domain,', 'roughly 12% of the word occurrences in client–operator interactions', 'were found to correspond to overlapping speech. Furthermore, this', 'study showed that dramatically higher WERs for both machine and', 'human transcription were obtained in regions where talkers overlap.', 'In general, multi-talker models attempt to improve speech recog-', 'nition from multiple overlapping speakers by decoding transcrip-', 'tions from each speaker, as opposed to improving speech recogni-', 'tion from a target speaker in the presence of background speech. The', 'A/V approach presented here assumes that audio and video signals', 'are available for overlapping talkers. Visual features, in the form of', 'mouth tracks aligned with speech, are similar to those used in A/V', 'end-to-end models for single-talker ASR [3', '', ']. Face tracking', 'and video processing are performed offline in this work using tools', 'described in [5].', 'In audio-only decoding of multiple transcriptions from overlap-', 'ping speech, there is a basic ambiguity associated with assigning', 'transcriptions to speakers [6', ']. However, it is well known that hu-', 'man listeners use visual information to disambiguate utterances from', 'multiple talkers in multi-party human-human interactions. This has', 'motivated the VCAM approach presented here. The VCAM model', 'incorporates the visual channel to assign each of the decoded tran-', 'scriptions to the face associated with each of the overlapping speak-', ""ers in the utterance. Assuming that a speaker's face serves as a proxy"", 'for speaker identity, this obviates the need to have a separate module', 'for dealing with the ambiguity associated with assigning transcrip-', 'tions to speakers.', 'The approach presented here builds on previous work in mask-', 'based end-to-end A/V multi-talker modeling [2', ']. The VCAM has', 'the same mask-based structure as used in the audio-only multi-talker', 'model in [2] where a conventional recurrent neural network trans-', 'ducer (RNN-T) architecture is extended to include a masking model', 'for separation of encoded audio features. It also includes multiple la-', 'bel encoders to encode transcripts from overlapping speakers. This', 'audio-only multi-talker model was extended in [8] to incorporate vi-', 'sual features from overlapping speakers with audio features. It was', 'shown to significantly improve speaker disambiguation in ASR on', 'overlapping speech relative to audio-only performance.', 'The input to the VCAM M-T model is an utterance containing', 'overlapping speech along with mouth-tracks associated with all on-', 'screen faces. A separate decoding pass is made for each available', 'mouth track. For a given decoding pass at each audio frame, an', 'attention weighted combination of encoded visual frames is input to', 'a mask model with the encoded audio frame. Visual frames from a', 'given mouth-track and audio frame are weighted according to their', 'similarity to the audio frame. It will be shown in Section 2 that the', 'attention maps produced by the model provide an indication when a', 'given speaker is talking even during speaker overlap intervals.', 'There has been a great deal of recent work on audio-only end-', 'to-end approaches to multi-talker ASR [9', '', ']. All of these ap-', 'proaches involve an extension of the single label encoder end-to-end', 'model with a training procedure that aligns overlapping speech with', 'transcriptions from multiple speakers. Further work has addressed', 'the latency issues associated with multi-talker decoding [11]. An', 'E2E A/V M-T approach has recently been applied to addressing the', 'multi-speaker cocktail party effect [12]. There is also a large body', 'of work on speech separation where the goal is to recover a target', 'speech signal in the presence of background speech [13', '', ']. This', 'includes approaches that fuse audio andvisual features for speech', 'separation in videos [15', '', ']. Explicit speech separation systems', 'generally optimize criteria related to signal-to-background distortion', 'and overall signal fidelity.', 'A simulated M = 2 speaker overlapping speech A/V corpus', 'was created for training A/V multi-talker models from a YouTube', 'A/V corpus that was originally created for training A/V ASR mod-', 'els [18', ']. Each utterance in the overlapped speech corpus was cre-', 'ated by combining two utterances with randomly selected overlap', 'intervals ranging from one to five seconds. This corpus is described', 'in more detail in Section 3. The results of an experimental study', 'is presented in Section 4. WERs for end-to-end multi-talker mod-', 'els are compared when evaluated on the simulated A/V overlapping', 'utterances described in Section 3.', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-10866']"
sardhaei22_interspeech	['Leibniz-Centre General Linguistics (Leibniz-ZAS), Berlin', 'Unitec Institute of Technology, Auckland, New Zealand', 'Humboldt Universitat zu Berlin, Berlin']
cao22b_interspeech	['Department of Electrical and Computer Engineering, University of Texas at Austin, USA', 'Department of Speech, Language, and Hearing Sciences, University of Texas at Austin, USA', 'School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA', 'Department of Speech, Language, and Hearing Sciences, University of Arizona, USA', 'Department of Otolaryngology, UT Southwestern Medical Center, USA', 'Department of Neurology, Dell Medical School, University of Texas at Austin, USA']
chan22b_interspeech	['Department of Linguistics, University of Pennsylvania, USA']
martikainen22_interspeech	['University of Twente, Human Media Interaction, Enschede, The Netherlands', 'Spotify, Stockholm, Sweden']
yadav22b_interspeech	['IIIT Delhi, India', 'J.P.Morgan AI Research, New York, USA', 'Carnegie Mellon University']
georges22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab', ' Grenoble, France']
sadekova22_interspeech	"[""Huawei Noah's Ark Lab""]"
wells22_interspeech	['The Centre for Speech Technology Research, University of Edinburgh']
singla22_interspeech	['Interactions, LLC, New Providence, NJ, USA']
vovk22_interspeech	"[""Huawei Noah's Ark Lab""]"
arora22_interspeech	['Carnegie Mellon University']
hori22_interspeech	['Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA']
wu22i_interspeech	['University of California, Berkeley, United States', 'Carnegie Mellon University, United States', 'University of Southern California, United States']
liu22w_interspeech	['School of Computer Science and Electronic Engineering, University of Surrey, UK', 'Speech, Audio, and Music Intelligence (SAMI) Group, ByteDance, China']
amid22_interspeech	['Google LLC']
baskar22b_interspeech	['ΦBrno University of Technology, †Technische Universit¨at Berlin']
tian22e_interspeech	['Indiana University Bloomington']
kataria22_interspeech	['Center for Language and Speech Processing', 'Human Language Technology Center of Excellence', 'Johns Hopkins University, Baltimore, MD, USA']
huo22_interspeech	['Siddhartha, Trevor Strohman, Francoise Beaufays', 'Google LLC, USA']
parikh22_interspeech	['University of Maryland, College Park, MD, USA', 'Alexa AI, Amazon.com Inc, Cambridge, MA, USA']
wang22aa_interspeech	['Music and Audio Research Laboratory, New York University, NY, USA', 'New Jersey Institute of Technology, Newark, NJ, USA']
huang22k_interspeech	['Google LLC']
lee22n_interspeech	['Laboratoire de Phonetique et Phonologie (UMR7018 CNRS/Univ. Sorbonne-Nouvelle, France)']
breiner22_interspeech	['Google Inc.']
farooq22_interspeech	['Speech and Hearing Research Group, University of Sheffield, UK.']
chen22q_interspeech	['Center for Robust Speech Systems, University of Texas at Dallas, TX 75080']
serdyuk22_interspeech	['Google', ' 8th Ave, New York', ' USA']
alenin22_interspeech	['ID R&D Inc., New York, USA']
maniati22_interspeech	['Innoetics, Samsung Electronics, Greece', 'Mobile Communications Business, Samsung Electronics, Republic of Korea']
nespoli22_interspeech	['Nuance Communications UK', 'Imperial College London']
parikh22b_interspeech	['Institute for Systems Research,', 'University of Maryland College Park, USA', 'Pindrop Inc., USA']
baird22_interspeech	['Christopher B. Gregory 1, Jacob Metrick 1, Garrett Boseck 1, Dacher Keltner 1,2, Alan S. Cowen 1', 'Hume AI Inc., New York City, New York, USA', 'University of California, Berkeley, California, USA']
boeddeker22_interspeech	['Paderborn University, Germany']
roux22_interspeech	['LS2N - Nantes University (France)', 'LIA - Avignon University (France)', 'LIUM - Le Mans University (France)']
kim22n_interspeech	['and', 'Hong-Goo Kang1', 'Department of Electrical Engineering, Yonsei University, Seoul, South Korea', 'Hyundai Motor Company, Seoul, South Korea']
su22b_interspeech	['M | M∗Modal']
chen22r_interspeech	['Pedro Moreno, Ankur Bapna, Heiga Zen', 'Google, Inc.']
jia22b_interspeech	['Google Research']
teytaut22_interspeech	['IRCAM, Sorbonne University, CNRS, UMR 9912 STMS, F-75004 Paris, France']
ablimit22_interspeech	['Cognitive Systems Lab, University of Bremen, Germany']
perez22_interspeech	['Taylor2, Reza Lotfian2, John Kane2, Emily Mower Provost1', 'Computer Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA', 'Cogito Corporation, Boston, Massachusetts, USA']
novak22_interspeech	['Amazon Alexa AI']
salais22_interspeech	['STMS Lab - IRCAM, Sorbonne Universite, CNRS, Ministere de la Culture, Paris, France', 'Lund University Cognitive Science, Lund University, Lund, Sweden']
jin22_interspeech	['Amazon Web Services, Palo Alto, CA, USA', 'Amazon Alexa AI, Sunnyvale, CA, USA']
brueggeman22_interspeech	['Center for Robust Speech Systems (CRSS), Cochlear Implant Processing Lab (CILab)', 'University of Texas at Dallas, Richardson, TX, USA']
xue22d_interspeech	['Microsoft Speech Group', 'Microsoft Translator Group']
albesano22_interspeech	['Nuance Communications, Inc.']
ekstedt22_interspeech	['KTH Speech, Music and Hearing', 'Stockholm, Sweden']
zhou22h_interspeech	['†Amazon Alexa, United States']
faria22_interspeech	['Mod9 Technologies, Berkeley, CA, USA']
baade22_interspeech	['Department of Computer Science, The University of Texas at Austin']
thakker22_interspeech	['Microsoft Corporation, One Microsoft Way, WA, USA']
thienpondt22_interspeech	['IDLab, Department of Electronics and Information Systems, Ghent University - imec, Belgium']
audibert22_interspeech	['Laboratoire de Phonetique et Phonologie, UMR7018 CNRS/University Sorbonne-Nouvelle,', 'Paris, France']
miller22_interspeech	['Department of Computer Science, The University of Texas at Austin', 'Austin, Texas', ', USA']
le22b_interspeech	['Department of Linguistics, University of Illinois Urbana-Champaign, USA', 'Department of East Asian Languages and Cultures, University of Illinois Urbana-Champaign, USA', 'Beckman Institute for Advanced Science and Technology, USA']
romana22_interspeech	['Computer Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA', 'Communication Sciences and Disorders, Northwestern University, Evanston, Illinois, USA']
peterson22_interspeech	['National Institute of Standards and Technology, USA', 'Dakota Consulting Inc., USA']
kirkland22_interspeech	['KTH Royal Institute of Technology']
panchapagesan22_interspeech	['Howard, Alex Park, James Walker, Alexander Gruenstein', 'Google LLC, U.S.A.']
joshi22_interspeech	['Piotr ˙Zelasko, Jesus Villalba, Sanjeev Khudanpur, Najim Dehak', 'Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD']
gent22_interspeech	['Department of Linguistics, University of Illinois Urbana-Champaign, USA', 'Beckman Institute for Advanced Science and Technology, USA', 'SRI International, USA']
joshi22b_interspeech	['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD']
cordero22_interspeech	"[""Universite Côte d'Azur, CNRS, BCL, France"", ""Complexity and Cognition Laboratory, Universite Côte d'Azur, Nice, France"", 'Universite Grenoble Alpes, CNRS, LPNC, Grenoble, France']"
siahkoohi22_interspeech	['School of Computational Science and Engineering, Georgia Institute of Technology', 'Chrome Media, Google', 'School of Engineering and Computer Science, Victoria University of Wellington']
shinohara22_interspeech	['Yahoo Japan Corporation, Japan', 'Carnegie Mellon University, USA']
wang22ba_interspeech	['Pedro Moreno Mengibar', 'Google 1']
zhu22e_interspeech	['University of Rochester', 'Adobe Research']
ashokumar22_interspeech	['Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France', 'Haskins Laboratories, New Haven, USA']
kim22o_interspeech	['Speech AI Lab., AI Center, NCSOFT Corp., Republic of Korea']
botelho22_interspeech	['INESC-ID/Instituto Superior Tecnico, University of Lisbon, Portugal', 'Cognitive Systems Lab (CSL), University of Bremen, Germany']
sawhney22_interspeech	['University of Marburg', ' BITS Pilani', ' BITS Pilani, K.K. Birla Goa Campus,', 'University of Maryland, College Park', ' Carnegie Mellon University']
lee22o_interspeech	['Amazon Music']
goncalves22_interspeech	['Multimodal Signal Processing (MSP) Lab., Department of Electrical and Computer Engineering', 'The University of Texas at Dallas, Richardson TX 75080, USA']
liu22x_interspeech	['Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK', 'Centre for Digital Music, Queen Mary University of London, UK', 'Speech, Audio, and Music Intelligence (SAMI) Group, ByteDance, China', 'Department of Computer Science and Engineering, The Ohio State University, USA']
nagamine22_interspeech	['Lancaster University']
zheng22d_interspeech	['Meta AI']
liu22y_interspeech	['DeLiang Wang3 Chuanzeng Huang2, Yuxuan Wang2', 'Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK', 'Speech, Audio, and Music Intelligence (SAMI) Group, ByteDance, China', 'Department of Computer Science and Engineering, The Ohio State University, USA']
teng22_interspeech	['Dept. of Computer Science, Vanderbilt University', 'Dept. of Otolaryngology–Head and Neck Surgery, Vanderbilt University Medical Center']
gao22e_interspeech	['University of Illinois at Urbana-Champaign', 'MIT-IBM Watson AI Lab 3University of California, Santa Barbara']
popuri22_interspeech	['Meta AI']
hwang22c_interspeech	['Google, U.S.A']
bittar22_interspeech	['Idiap Research Institute, Martigny, Switzerland', 'Ecole Polytechnique Federale de Lausanne, Switzerland']
yuan22b_interspeech	['Carnegie Mellon University, United States']
wang22ca_interspeech	['Phonetics and Speech Laboratory', 'School of Linguistic, Speech and Communication Sciences', 'Trinity College Dublin, Ireland']
yang22v_interspeech	['Center for Robust Speech Systems (CRSS), University of Texas at Dallas, Richardson, TX, USA', 'Northern Arizona University, Flagstaff, AZ, USA', 'Pennsylvania State University, State College, PA, USA']
chou22_interspeech	['Multimodal Signal Processing (MSP) lab, Department of Electrical and Computer Engineering', 'The University of Texas at Dallas, Richardson TX 75080, USA', 'Department of Electrical Engineering, National Tsing Hua University, Taiwan']
kothare22_interspeech	['Modality.AI, Inc., San Francisco, CA, USA', 'Purdue University, West Lafayette, IN, USA', 'University of California, San Francisco, CA, USA']
hard22_interspeech	['Google LLC, Mountain View, CA, U.S.A.', 'University of Washington, Seattle, WA, U.S.A.']
kilgour22_interspeech	['Google Research']
chen22s_interspeech	['Amazon Alexa AI, USA']
trinh22_interspeech	['Amazon Alexa AI, USA']
miao22_interspeech	['National Institute of Informatics, Japan 2LIA, University of Avignon, France']
ploujnikov22_interspeech	['Mila - Quebec AI Institute', 'Universite de Montreal', 'Concordia University']
raju22_interspeech	['Amazon Alexa AI, USA']
liu22z_interspeech	['MIT CSAIL 2Meta AI']
bakhturina22_interspeech	['NVIDIA, Santa Clara, USA']
bando22_interspeech	['National Institute of Advanced Industrial Science and Technology, Japan', 'Tokyo Institute of Technology, Japan']
nidadavolu22_interspeech	['Amazon Alexa']
huang22l_interspeech	['LG Electronics Toronto AI Lab', 'LG Electronics Artificial Intelligence Lab']
vuong22_interspeech	['Department of Electrical and Computer Engineering, Carnegie Mellon University, USA', 'Language Technologies Institute, Carnegie Mellon University, USA']
bu22_interspeech	['Dept. of Electrical Engineering and Computer Science, University of Missouri-Columbia, USA']
dhamyal22_interspeech	['Language Technologies Institute, Carnegie Mellon University']
virkar22_interspeech	['AWS AI Labs', 'Alexa AI']
gharbieh22_interspeech	['LG Electronics Toronto AI Lab', 'LG Electronics Artificial Intelligence Lab']
yang22w_interspeech	['Carnegie Mellon University, Pittsburgh, PA, USA']
sadhu22_interspeech	['Center for Language and Speech Processing, Johns Hopkins University, USA', 'Human Language Technology Center of Excellence, Johns Hopkins University, USA']
shao22c_interspeech	['Center of Language and Speech Processing,', 'The Johns Hopkins University, Baltimore, MD 20218, USA']
seneviratne22_interspeech	['University of Maryland - College Park, USA']
patterson22_interspeech	['Google Research, Cambridge MA']
lee22p_interspeech	['School of Electrical Engineering, KAIST, Daejeon, Republic of Korea']
mei22_interspeech	['Centre for Vision, Speech and Signal Processing (CVSSP)', 'University of Surrey, UK']
udagawa22b_interspeech	['IBM Research - Tokyo, Japan', 'IBM T. J. Watson Research Center, Yorktown Heights, USA']
chung22_interspeech	['Department of Computer Science and Engineering, The Hong Kong University of Science and', 'Technology, Clear Water Bay, Hong Kong', 'Logistics and Supply Chain MultiTech R&D Centre, Pok Fu Lam, Hong Kong']
carne22_interspeech	['Speech and Language Laboratory, the Australian National University']
fan22d_interspeech	['Dept. of Electrical and Computer Engineering, University of California, Los Angeles, USA']
yoneyama22_interspeech	['Nagoya University, Japan']
saba22_interspeech	['Center for Robust Speech Systems (CRSS), Cochlear Implant Processing Lab (CILab),', 'Erik Jonsson School of Engineering & Computer Science,', 'University of Texas at Dallas; Richardson, Texas, USA']
yoon22b_interspeech	['NAVER Corp., Seongnam, Korea', 'LINE Corp., Tokyo, Japan']
saraf22_interspeech	['Pindrop, Atlanta, USA']
tam22_interspeech	['AWS AI Labs']
yamamoto22_interspeech	['Doctoral Program in Informatics, University of Tsukuba, Japan', 'Graduate School of Culture Technology, KAIST, South Korea']
sustek22_interspeech	['Brno University of Technology, Czechia', 'Center for Language and Speech Processing, Johns Hopkins University, USA', 'Human Language Technology Center of Excellence, Johns Hopkins University, USA']
cho22c_interspeech	['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA', 'Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD,', 'USA']
kim22p_interspeech	['Meta AI, USA']
kaneko22_interspeech	['NTT Communication Science Laboratories, NTT Corporation, Japan']
markovic22_interspeech	['Meta, Reality Labs Research, Pittsburgh PA, USA', 'Meta AI Research, Paris, France']
zheng22e_interspeech	"[""Huawei Noah's Ark Lab, Shenzhen, China""]"
zhang22ca_interspeech	['Department of Computer Science and Engineering, The Ohio State University, USA', 'Center for Cognitive and Brain Sciences, The Ohio State University, USA']
nayak22_interspeech	['Sigtia1, Erik Marchi1, Varun Lakshminarasimhan1, Minsik Cho1, Saurabh Adya1, Chandra Dhir3∗,', 'Ahmed Tewfik1', 'Apple', 'Department of Computer Science, The University of Hong Kong', 'JPMorgan Chase & Co.']
yang22x_interspeech	['Carnegie Mellon University', 'Meta Reality Labs Research']
siriwardena22_interspeech	['University of Maryland College park, MD, USA', 'Pindrop, GA, USA']
qi22_interspeech	['Nara Institute of Science and Technology, Japan', 'Japan Advanced Institute of Science and Technology, Japan']
lei22c_interspeech	['Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Huya Inc., Guangzhou, China', 'XVerse Inc., Shenzhen, China', 'The Chinese University of Hong Kong, Hong Kong SAR, China']
xie22b_interspeech	['Center for Robust Speech Systems (CRSS), University of Texas at Dallas, TX']
bai22c_interspeech	['Department of Computer Science and Engineering, Southern University of Science and Technology', 'ByteDance AI Lab']
dumpala22_interspeech	['Vector Institute', 'Dalhousie University and 3 Nova Scotia Health, Canada']
meripo22_interspeech	['Abridge AI Inc.']
li22ba_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Department of Electrical and Computer Engineering, National University of Singapore, Singapore', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
faridee22_interspeech	['Information Systems, University of Maryland, Baltimore County, MD, USA', 'Microsoft Research Labs, Redmond, WA, USA']
wei22f_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
wang22da_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
kumar22c_interspeech	['Ramakrishnan1, Pawan Goyal 3, Preethi Jyothi1', 'IIT Bombay, Mumbai, India', 'Uniphore', 'IIT Kharagpur, WB, India']
wang22ea_interspeech	['School of Computer Science, Wuhan University, Wuhan, China', 'Data Science Research Center, Duke Kunshan University, Kunshan, China', 'Guangdong OPPO Mobile Telecommunications Corp., Ltd., Guangzhou, China']
peng22d_interspeech	['Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University', 'Hsinchu, Taiwan', 'Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan']
ogushi22_interspeech	['Graduate School of Integrated Basic Sciences, Nihon University', 'College of Humanities and Sciences, Nihon University', 'NTT Human Informatics Laboratories, NTT Corporation']
morshed22_interspeech	['University of Illinois Urbana-Champaign, Urbana, Illinois 61801']
nallanthighal22_interspeech	['Philips Research, Eindhoven, The Netherlands', 'Centre for Language Studies (CLS), Radboud University Nijmegen']
yu22b_interspeech	"[""Northwestern Polytechnical University, Xi'an, China"", 'College of Computer Science and Technology, Zhejiang University, Hangzhou, China']"
an22_interspeech	['Speech Processing and Machine Intelligence (SPMI) Lab, Tsinghua University, China1', 'Meituan, China2']
pandey22c_interspeech	['Reality Labs Research at Meta, Redmond, WA USA', 'Department of Computer Science and Engineering, The Ohio State University, USA']
sudo22_interspeech	['Honda Research Institute Japan Co., Ltd., Saitama, JAPAN', 'Department of Systems and Control Engineering, School of Engineering, Tokyo Institute of', 'Technology, Tokyo, JAPAN', 'Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA']
begus22_interspeech	['University of California, Berkeley']
parikh22c_interspeech	['Institute for Systems Research', 'University of Maryland College Park, USA', 'ENS Paris, PSL Universite, France']
li22ca_interspeech	['Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems,', 'Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Department of Systems Engineering and Engineering Management,', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China']
lian22_interspeech	['UC Berkeley, EECS, CA 2 Tencent AI Lab, Bellevue, WA']
garg22_interspeech	['Ahmed H. Abdelaziz1, Erik Marchi1, Saurabh Adya1, Chandra Dhir2†, Ahmed Tewfik1', 'Apple', 'JPMorgan Chase & Co.']
kameoka22_interspeech	['NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation']
lian22b_interspeech	['UC Berkeley, CA 2 Carnegie Mellon University, PA 3 University of Southern California, CA']
mun22_interspeech	['Samsung Research, Seoul, South Korea']
ghosh22c_interspeech	['Speech Lab, Department of Electrical Engineering, IIT Madras,', 'Cisco Systems', 'Adobe Media Data Science Research,', 'IIIT-Delhi', ' SUNY at Buffalo']
belitz22_interspeech	['Center for Robust Speech Systems (CRSS), Robust Speech Technologies Lab (RSTL), Erik', 'Jonsson School of Engineering & Computer Science, University of Texas at Dallas, Richardson,', 'Texas, U.S.A.']
tseng22_interspeech	['Graduate Institute of Communication Engineering, National Taiwan University']
tseng22b_interspeech	['Graduate Institute of Communication Engineering, National Taiwan University']
delcroix22_interspeech	['Marc Delcroix1, Keisuke Kinoshita1, Tsubasa Ochiai1, Katerina Zmolikova2, Hiroshi Sato1,', 'Tomohiro Nakatani1']
miao22b_interspeech	['Ping An Technology']
agarwal22_interspeech	['$LEAP lab, Indian Institute of Science, Bangalore, India.', '∗Sony Group Corporation, Tokyo, Japan.']
jun22_interspeech	['University of California Los Angeles, USA', 'University of Southern California, USA']
dissanayake22_interspeech	['Nanayakkara1', 'Augmented Human Lab, Auckland Bioengineering Institute, The University of Auckland, New', 'Zealand. 2Transport, Health and Urban Design Research lab, Melbourne School of Design, The', 'University of Melbourne, Australia.']
miao22c_interspeech	['Ping An Technology']
chien22b_interspeech	['KenKone Medical Co., Ltd., Taiwan', 'National Taiwan University of Science and Technology, Taiwan']
rajchetupalli22_interspeech	['LEAP lab, Electrical Engineering, Indian Institute of Science, Bangalore, India.']
shi22e_interspeech	['Graduate School of Informatics, Kyoto University, Kyoto, Japan', 'Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'National Institute of Information and Communications Technology, Kyoto, Japan', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
jung22c_interspeech	['Naver Corporation, South Korea,', 'EURECOM, Sophia Antipolis, France,', 'School of Computer Science, University of Seoul, South Korea,', 'University of Eastern Finland, Finland']
meng22c_interspeech	['Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems,', 'Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Department of Systems Engineering and Engineering Management,', 'The Chinese University of Hong Kong, Hong Kong SAR, China', 'Tencent, Shanghai, China']
komatsu22_interspeech	['LINE Corporation', 'NAVER Corporation,', 'Carnegie Mellon University']
kothapally22b_interspeech	['Center for Robust Speech Systems (CRSS), The University of Texas at Dallas, TX, USA']
terashima22_interspeech	['LINE Corp.,Tokyo, Japan,', 'NAVER Corp., Seongnam, Korea']
shen22b_interspeech	['National Institute of Information and Communications Technology (NICT)']
takashima22_interspeech	['Hitachi, Ltd. Research & Development Group, Japan', 'Language Technologies Institute, Carnegie Mellon University, USA', 'Center for Language and Speech Processing, Johns Hopkins University, USA']
nakagome22_interspeech	['LINE Corporation']
liesenfeld22_interspeech	['Centre for Language Studies', 'Radboud University']
mukherjee22_interspeech	['Microsoft STC India']
velichko22_interspeech	['St. Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences,', 'St. Petersburg Federal Research Center of the Russian Academy of Sciences (SPC RAS), Russia', 'Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands', 'ITMO University, St. Petersburg, Russia']
chen22t_interspeech	['Kuaishou Technology, Beijing, China']
mohammadamini22_interspeech	"[""LIA (Laboratoire Informatique d'Avignon),"", 'University of Lorraine, CNRS, Inria, Loria, F-54000, Nancy, France']"
li22da_interspeech	['ARC Lab, Tencent PCG']
takashima22b_interspeech	['NTT Corporation, Japan']
peng22e_interspeech	['Peking University, ECE, China']
hong22_interspeech	['KAIST, Daejeon, South Korea', 'Genesis Lab Inc., Seoul, South Korea']
ashihara22_interspeech	['NTT Corporation, Japan']
guillaume22_interspeech	"['Langues et Civilisations a Tradition Orale (LACITO), CNRS – Universite Sorbonne Nouvelle –', 'Institut National des Langues et Civilisations Orientales (INALCO)', 'Universite de Paris Cite, Laboratoire de Linguistique Formelle (LLF), CNRS, F-75013 Paris,', 'France', ""Laboratoire d'Informatique de Grenoble (LIG), CNRS – Universite Grenoble Alpes – Grenoble"", 'INP - Institut national de recherche en informatique et en automatique (INRIA)', 'Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab', ' Grenoble, France', ""Centre de Recherches Linguistiques sur l'Asie Orientale (CRLAO), CNRS – Ecole des Hautes"", 'Etudes en Sciences Sociales – Institut National des Langues et Civilisations Orientales (INALCO)']"
rouhe22_interspeech	['Aalto University, Department of Signal Processing and Acoustics, Finland']
zhang22ea_interspeech	['College of Intelligence and Computing,Tianjin University, China', 'Faculty of Intelligence and Informatics, Konan University, Japan']
stoidis22_interspeech	['Centre for Intelligent Sensing, Queen Mary University of London, UK']
gao22f_interspeech	['University of Groningen, Campus Fryslˆan, Leeuwarden, the Netherlands']
he22d_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
lemercier22_interspeech	['⋆Signal Processing (SP), Universit¨at Hamburg, Germany', '†Advanced Bionics, Hanover, Germany']
arunkumar22_interspeech	['Speech Lab, Indian Institute of Technology Madras']
singh22b_interspeech	['Level AI, Delhi', 'Institute for Infocomm Research (I2R), A*STAR, Singapore', '† Samsung R&D Institute, Bangalore', '⋆Birla Institute of Technology & Science, Pilani']
meng22d_interspeech	"['Department of Biomedical Engineering, The University of Melbourne', 'Department of Brain and Cognitive Engineering, Korea University', ""Department of Medicine, St Vincent's Hospital, The University of Melbourne"", 'Department of Artificial Intelligence, Korea University', 'Graeme Clark Institute for Biomedical Engineering, The University of Melbourne']"
wang22fa_interspeech	['Alibaba Group,2Boston University']
wang22ga_interspeech	['School of Informatics, Xiamen University, China', 'Center for Speech and Language Technologies, Tsinghua University, China', 'School of Electronic Science and Engineering, Xiamen University, China']
chang22h_interspeech	['GLAM – Group on Language, Audio, & Music, Imperial College London, United Kingdom', 'L3S Research Center, Leibniz Universit¨at Hannover, Germany', 'Griffith University, Australia', 'EIHW – Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany']
xu22h_interspeech	['CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China']
masumura22_interspeech	['NTT Corporation, Japan']
zhang22fa_interspeech	['University of Sheffield, Department of Computer Science, Sheffield, UK', 'Toshiba Cambridge Research Laboratory, Cambridge, UK']
das22_interspeech	['Georgia Institute of Technology, USA']
tachibana22_interspeech	['PKSHA Technology Inc., Hongo, Bunkyo City, Tokyo, Japan', 'Asia University, Sakai, Musashino City, Tokyo, Japan']
zheng22f_interspeech	['Cambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.']
vandermerwe22_interspeech	['MediaLab, E&E Engineering, Stellenbosch University']
bhanushali22_interspeech	['Kumar3, Adithya Raj Kolladath1, Nithya Ravi1, Aaditeshwar Seth4, Ashish Seth1, Abhayjeet Singh3,', 'Vrunda N. Sukhadia1, Umesh S1, Sathvik Udupa3 and Lodagala V. S. V. Durga Prasad1', 'Department of Electrical Engineering, Indian Institute of Technology, Madras-600036, India.', 'Uliza CEO, Co-founder, City of Cape Town, Western Cape, South Africa.', 'Electrical Engineering Department, Indian Institute of Science (IISc), Bangalore-560012, India.', 'Khosla School of Information Technology, Indian Institute of Technology, New Delhi-110016,', 'India.']
zeng22b_interspeech	['School of Public Health, Zhejiang University, Hangzhou, China', 'Peking University, China']
arunkumar22b_interspeech	['Speech Lab, Dept. of Electrical Engineering, IIT Madras, Chennai, India']
zhu22f_interspeech	['JD AI, Beijing, China', 'LTL, University of Cambridge']
rennie22_interspeech	['University of Glasgow, G12 8QQ Glasgow, United Kingdom', 'Intel, Santa Clara, US']
fukuda22b_interspeech	['Nara Institute of Science and Technology, Japan']
ye22b_interspeech	['Ping An Technology (Shenzhen) Co., Ltd.', 'University of Science and Technology of China']
aroudi22_interspeech	['Sony Europe B.V., Stuttgart, Germany']
ochi22_interspeech	['Graduate School of Informatics, Kyoto University, Kyoto, Japan', 'Department of Computer Science, Tokyo Metropolitan University, Hino, Japan', 'Faculty of Medicine, University of Tokyo, Tokyo, Japan,', 'Professor Emeritus, University of Tokyo, Tokyo, Japan,', 'Department of Psychiatry, Hamamatsu University School of Medicine, Hamamatsu, Japan']
handekabil22_interspeech	['Idiap Research Institute, Martigny, Switzerland', 'Ecole polytechnique federale de Lausanne (EPFL), Lausanne, Switzerland']
wu22j_interspeech	['NVIDIA']
meeus22_interspeech	['KU Leuven, Dept. Computer Sciences CS-LIIR', 'KU Leuven, Dept. Electrical Engineering ESAT-PSI', 'ABSTRACT', 'We explore the benefits that multitask learning offer to speech', 'processing as we train models on dual objectives with au-', 'tomatic speech recognition and intent classification or sen-', 'timent classification. Our models, although being of modest', 'size, show improvements over models trained end-to-end on', 'intent classification. We compare different settings to find the', 'optimal disposition of each task module compared to one an-', 'other. Finally, we study the performance of the models in low-', 'resource scenario by training the models with as few as one', 'example per class. We show that multitask learning in these', 'scenarios compete with a baseline model trained on text fea-', 'tures and performs considerably better than a pipeline model.', 'On sentiment classification, we match the performance of an', 'end-to-end model with ten times as many parameters. We', 'consider 4 tasks and 4 datasets in Dutch and English.', 'Index Terms— Spoken Language Understanding, Trans-', 'formers, Speech Processing, Multitask Learning, Pretraining', '. INTRODUCTION', 'Spoken language understanding (SLU) generally follows a', 'pipeline approach, where an automatic speech recognition', '(ASR) system is followed by a natural language understand-', 'ing (NLU) module. These systems know multiple limitations,', 'including cascading errors from one module to the next or dis-', 'carding relevant information related to speech not captured', 'by text. This motivated research directed towards end-to-end', 'SLU that removed the explicit speech transcription required', 'by older systems [1', ']. End-to-end approaches concerned', 'only with SLU tasks, however, struggle to match the perfor-', 'mance of pipeline systems. Indeed, these specific datasets', 'are considerably smaller and neural networks notably require', 'large amounts of data to perform well. To circumvent this', 'data scarcity issue, pretraining and multitask learning offer', 'an elegant solution. Pretraining allows applying what a neu-', 'ral network learned from a large dataset to another domain', 'by partly or entirely initializing the weights in the model of', 'This research received funding from the Flemish Government under', 'the “Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen” pro-', 'gramme.', 'interest with the weights from the pretrained model. If the', 'tasks share certain characteristics with the ultimate objective', 'and the domain shift between the datasets is not too big,', 'we can decrease considerably the amount of training exam-', 'ples and computation resources necessary to train the final', 'model. During the actual training or finetuning of the model,', 'a trade-off appears, as further training will inevitably lead', 'to a specialization in favor of the new task and catastrophic', 'forgetting of what was learned during the pretraining phase.', 'In contrast, with multitask learning, the tasks are trained', 'in parallel. In this case, a similar trade-off occurs, caused', 'by competing optimization objectives. This approach relies', 'strongly on the assumption that the multiple objectives share', 'a common ground and that the ability to solve one helps on', 'solving others. It is easy to imagine many examples for which', 'this assumption might hold or not, depending on the tasks and', 'datasets considered. In this work, we investigate synergies', 'between spoken language understanding and speech recogni-', 'tion. One task of interest is intent classification (IC), where a', 'system must identify an intent with any number of arguments.', 'A second task investigated here is sentiment classification,', 'where the goal is to determine whether a spoken utterance', 'expresses a positive, negative or neutral sentiment. Having a', 'system able to transcribe speech to text is likely to be helpful', 'for such tasks and requires fewer data and fewer parameters', 'to reach a good performance [3', ']. These are two aspects that', 'we will focus on in this work: We compare a multitask model', 'optimized for both speech transcription and intent recognition', 'or sentiment classification with a model pretrained on ASR', 'then finetuned on SLU. We also look into edge cases in IC by', 'simulating training sets with few examples. Further, we limit', 'our model sizes considerably for the advantages in terms of', 'speed and energy consumption that it brings. Experiments', 'are performed in English and in Dutch.', '. ARCHITECTURE', 'The model is composed of a generic transformer encoder on', 'which we stack different modules, each specialized for one', 'task. We experiment with different arrangements in order to', 'find the optimal architecture that allows each task to bene-', 'fit from - rather than compete with - each other. We have', 'Interspeech 2022', '-22 September 2022, Incheon, Korea', 'Copyright © 2022 ISCA', '.21437/Interspeech.2022-11401']
kinoshita22_interspeech	['NTT corporation, Japan', 'Paderborn University, Germany']
wang22ha_interspeech	['School of Electronic Science and Engineering, Xiamen University, China', 'School of Informatics, Xiamen University, China', 'College of Ocean and Earth Sciences, Xiamen University', 'Key Laboratory of Underwater Acoustic Communication and', 'Marine Information Technology (Xiamen University), Minister of Education']
tanaka22_interspeech	['NTT Corporation']
xu22i_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
moriya22_interspeech	['NTT Corporation, Japan', 'Tokyo Institute of Technology, Japan']
song22e_interspeech	['Tianjin Key Laboratory of Cognitive Computing and Application,', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Department of Electrical and Computer Engineering, National University of Singapore, Singapore', 'Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan', 'Japan Advanced Institute of Science and Technology, Ishikawa, Japan']
takeuchi22_interspeech	['NTT Corporation, Japan']
li22ea_interspeech	['Kuaishou Technology Co. Beijing, China']
maekaku22_interspeech	['Yahoo Japan Corporation, Tokyo, JAPAN', 'Carnegie Mellon University, PA, USA']
farooq22b_interspeech	['Speech and Hearing Research Group, University of Sheffield, UK.']
kang22d_interspeech	['Ping An Technology (Shenzhen) Co., Ltd.']
lee22q_interspeech	['Department of Intelligence and Information, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University', 'AI Institute, Seoul National University']
zhao22l_interspeech	['School of Information and Electronic Engineering, Zhejiang University, China']
liu22aa_interspeech	['School of Information Science and Technology, Qingdao University of Science and Technology,', 'Qingdao 266061, China']
hou22c_interspeech	['WAVES Research Group, Ghent University, Belgium']
badi22_interspeech	['Deep Machine Lab (DMLAB)', 'Department of Computer Science and Engineering, Korea University']
nguyen22e_interspeech	['VinAI Research, Vietnam']
nicmanis22_interspeech	['Tilde SIA', 'Vienibas gatve 75a, Riga, Latvia', 'Faculty of Computing, University of Latvia', 'Raina bulvaris 19, Riga, Latvia']
xu22j_interspeech	"['Krug3, Santitham Prom-on4, Lorna F. Halliday1,5', 'Department of Speech Hearing and Phonetic Sciences, University College London, UK', 'Faculty of Electrical Engineering and Information Technologies, UCMS, Skopje, RN Macedonia', 'Institute of Acoustics and Speech Communication, Technische Universitat, Dresden, Germany', ""Computer Engineering Department, King Mongkut's University of Technology Thonburi,"", 'Thailand', 'Medical Research Council (MRC) Cognition and Brain Sciences Unit, University of Cambridge,', 'Cambridge, UK']"
arco22_interspeech	['Artificial Intelligence Lab, Vrije Universiteit Brussel, Brussels, Belgium', 'Universidad de Holguin, Holguin, Cuba']
tan22c_interspeech	['Institute for Infocomm Research, A*STAR, Singapore']
draxler22_interspeech	['Institute of Phonetics and Speech Processing', 'Ludwig Maximilian University Munich', 'Germany']
ingle22_interspeech	['Observe.AI, India']
park22e_interspeech	['NVIDIA']
zielinski22_interspeech	['UHURA Technologies', 'University of Warsaw, Faculty of Psychology, Human Interactivity and Language Lab', 'Graz University of Technology, Signal Processing and Speech Communication Laboratory']
jin22b_interspeech	['Beijing Language and Culture University, Beijing, China']
ronssin22_interspeech	['Logitech Europe S.A.', ', Lausanne, Switzerland']
paul22_interspeech	['Meta Platforms, Inc.']
ivanko22_interspeech	['St. Petersburg Federal Research Center of the Russian Academy of Sciences (SPC RAS),', 'Saint Petersburg, Russia', 'ITMO University, Saint Petersburg, Russia']
schafer22_interspeech	['A. Abad4 M. Schuster3, T. Arias-Vergara1,2,3‡', 'Pattern Recognition Lab. Friedrich-Alexander Universit¨at, Erlangen-Nurnberg, Germany', 'GITA Lab, Universidad de Antioquia, Calle 70 No. 52-21, Medellin, Colombia', 'Department of Otorhinolaryngology, Head and Neck Surgery, Ludwig-Maximilians', 'University, Munich, Germany', 'INESC-ID, Portugal']
vandevreken22_interspeech	['Institute for Language, Cognition and Computation (ILCC), University of Edinburgh', 'Centre for Speech Technology Research (CSTR), University of Edinburgh']
haider22_interspeech	['Usher Institute of Population Health Sciences & Informatics', 'Edinburgh Medical School, the University of Edinburgh, UK']
siddarth22_interspeech	['Indian Institute of Information Technology, Design and Manufacturing, Kancheepuram, India', 'Indian Institute of Science (IISc), Bangalore-560012, India']
lin22e_interspeech	['Columbia University']
bhattacharya22b_interspeech	['Gonuguntla3, Murali Alagesan4', 'LEAP lab, Indian Institute of Science, Bangalore, India', 'Ramaiah Medical College Hospital,', 'Bangalore, India', 'General Hospital, Hoskote, Bangalore, India', 'PSG Institute of Medical Sciences', 'and Research, India']
kumar22d_interspeech	['Indian Institute of Technology, Bombay', 'Indian Institute of Information Technology, Una']
